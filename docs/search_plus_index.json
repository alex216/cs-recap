{"./":{"url":"./","title":"Computer Science Recapitulaion","keywords":"","body":"CS Recap ここではComputer Scienceに関連する分野で自分が勉強した内容を書いていきます 間違った内容などがあったら是非青い鳥にDM or リプライをするかプルリクエストをして下さい 文中ではOSをUbuntu(18.04以降)かMac OSを(Mojave以降)を想定しています 諸々 About editing About me 本文に従って何かしらの不利益を被った際の責任は取りませんのであしからず Core CSの基礎となるような内容です Math Calculus Linear algebra Statistics Set theory Topology Physics Electromagnetics CS Computational theory Algorithm Computer architecture Signal processing C language Selective Coreの内容を土台とする発展的な内容です Information theory Operating system Other topics 関連した内容です Editor Latex Git "},"EDIT.html":{"url":"EDIT.html","title":"About Editing","keywords":"","body":"Editing 主に自分用にまとめています 目次の書き方 読んで勉強する目的が分かり,一望ができるような目次にする それぞれの分野に関して # Title * [chap1] * [chap2] . . . 記事の書き方 概要的な説明に留める 細かい証明は基本的には省く \"Done is better than perfect.\" 構成 # Title ## 小見出し ### 小見出しに関連する話 細かいところ mathjaxをdisplay形式で書くとコード文内でドル記法が使えなくなるので注意 plantumlは手元が書ける graphvizは手元でdotファイルを書いて生成したsvgを埋め込んでいる "},"ABOUT.html":{"url":"ABOUT.html","title":"About Me","keywords":"","body":"About me ここを読めば自分の経歴がわかります このブログでは雑多なことを話しています Twitterは最近使っていないですが何かあったら反応しやすいです このサイトを書く動機 勉強したことをまとめるのは自分の勉強にもなる 特に人に教えるように話すことは学びに繋がると感じている 間違いを勝手に見て勝手に直してくれる人がいる 同じような勉強をしている人の参考になるかもしれない 体系的にCS関連のことを書いたブログがないのでもしかしたら需要があるかも知れない なくても検索で出てきて参考になってくれるかもしれない "},"cal/cal00.html":{"url":"cal/cal00.html","title":"Calculus","keywords":"","body":"Calculus 微分では局所的な変化,積分では大域的な累積に関して扱います Real number & Convergence "},"cal/cal01.html":{"url":"cal/cal01.html","title":"Real number & Convergence","keywords":"","body":"Real number & Convergence 数の分類 自然数 自然数は英語でnatural numberなので自然数の集合はN\\mathbb{N}Nと表します N={1,2,3,4,⋯}\\mathbb{N}=\\lbrace 1,2,3,4,\\cdots \\rbraceN={1,2,3,4,⋯}となります 整数 整数は自然数に0や負の数を加えてもので,ドイツ語でganze Zahlと呼ぶので集合はZ\\mathbb{Z}Zと表します Z={⋯,−2,−1,0,1,2,⋯}\\mathbb{Z} = \\lbrace \\cdots,-2,-1,0,1,2,\\cdots \\rbraceZ={⋯,−2,−1,0,1,2,⋯}となります 有理数 有理数はrational numberなので,その集合はR\\mathbb{R}Rと表します rationalはratioから来た言葉であり,分数で表せる数が有理数です 循環する小数は分数で表すことができるので,有理数です 有理数の集合はQ\\mathbb{Q}Qと書きます (厳密ではない)実数 循環しない小数は分数で表せません.このような数を無理数と言い,無理数と有理数の集合の和集合を実数と言います 実数の集合はR\\mathbb{R}Rと書きます 実数 実数全体の集合をR\\mathbb{R}Rとし,S⊆RS\\subseteq \\mathbb{R}S⊆Rとします ∀s∈S\\forall{s} \\in S∀s∈Sに対して,s⊆Ss \\subseteq Ss⊆Sとなる実数aaaがあれば,aaaはSSSの下界と呼びます.また,下界が存在することを下に有界と言います 逆は上界,上に有界と言います 下界で最大の数を下限,上界で最小の数を上限と言います 数列と収束性 数列 数列はa1,a2,a3,⋯a_1, a_2, a_3, \\cdotsa1​,a2​,a3​,⋯と続く数の列のことです n番目の元ana_nan​を一般項と考え,数列{an}\\lbrace a_n \\rbrace{an​}と書くことも有ります 数列の部分集合を同じ順番に並べたものを部分列と呼びます 収束 {an}\\lbrace a_n \\rbrace{an​}がnnnが大きくなるに従って,ある数α\\alphaαに近づくとき,{an}\\lbrace a_n \\rbrace{an​}はα\\alphaαに収束すると言います.また,α\\alphaαは{an}\\lbrace a_n \\rbrace{an​}の極限と言います.これを以下のように書きます lim⁡n→∞an=α \\lim_{n \\to \\infty}a_n = \\alpha n→∞lim​an​=α もしくは an→α(n→∞) a_n \\to \\alpha \\;\\;\\;(n \\to \\infty) an​→α(n→∞) と書きます n→∞n \\to \\inftyn→∞でana_nan​が限りなく大きくなるとき,{an}\\lbrace a_n \\rbrace{an​}は発散するといい,次のように書きます lim⁡n→∞an=∞ \\lim_{n \\to \\infty}a_n = \\infty n→∞lim​an​=∞ もしくは an→∞(n→∞) a_n \\to \\infty \\;\\;\\;(n \\to \\infty) an​→∞(n→∞) また, lim⁡n→∞an=−∞ \\lim_{n \\to \\infty}a_n = -\\infty n→∞lim​an​=−∞ もしくは an→−∞(n→∞) a_n \\to -\\infty \\;\\;\\;(n \\to \\infty) an​→−∞(n→∞) ±∞\\pm \\infty±∞に発散していなくても,どこにも収束しない時,発散するということもあります コーシー列 数列がどこに収束しているかはわからないが,どこかに収束しているときにコーシー列という概念が役に立ちます m,nm,nm,nが限りなく大きくなるにつれ,∣am−an∣|a_m - a_n|∣am​−an​∣が000に近づくとき,{an}\\lbrace a_n \\rbrace{an​}をコーシー列と言います 収束の厳密な定義 数列{an}\\lbrace a_n \\rbrace{an​}がα\\alphaαに収束することは,任意の正数ε\\varepsilonεに対し,自然数NNNが存在し,全てのn≥Nn \\geq Nn≥Nに対し ∣an−α∣ε |a_n - \\alpha| ∣an​−α∣ε となることです.論理学の記号を使うと ∀ε>0,∃N;∣an−α∣ε,∀n≥N \\forall{\\varepsilon} >0,\\;\\; \\exists{N}; \\;\\;\\; |a_n - \\alpha| ∀ε>0,∃N;∣an​−α∣ε,∀n≥N と書けます 実数の完備性 ボルツァーノ･ワイヤシュトラスの定理 級数 "},"linear-alge/linear-alge00.html":{"url":"linear-alge/linear-alge00.html","title":"Linear algebra","keywords":"","body":"Linear algebra "},"stat/stat00.html":{"url":"stat/stat00.html","title":"Statistics","keywords":"","body":"Statistics "},"set/set00.html":{"url":"set/set00.html","title":"Set theory","keywords":"","body":"Set theory "},"topo/topo00.html":{"url":"topo/topo00.html","title":"Topology","keywords":"","body":"Topology "},"elec/elec00.html":{"url":"elec/elec00.html","title":"Electromagnetics","keywords":"","body":"Electromagnetics 多くのコンピュータの動力源である電気と磁気についての理論 Basic Math Coulomb & Gaussian Potential "},"elec/elec01.html":{"url":"elec/elec01.html","title":"01 Basic Math","keywords":"","body":"01 Basic Math 電磁気で用いる数学についてやります ベクトル解析 電磁気では多くの数学が使われます ここではそのうち最も重要であるベクトル解析について簡単に触れます スカラー積とベクトル積 スカラー積は内積,ベクトル積は外積とも呼びます ベクトルA\\bm{A}AとベクトルB\\bm{B}Bのスカラー積とは,A\\bm{A}Aの大きさにB\\bm{B}BのA\\bm{A}Aのに落とした正射影の成分の大きさをかけたものです 式では次のように書きます A⋅B=∣A∣∣B∣cos⁡θ \\bm{A} \\cdot \\bm{B} = |A||B|\\cos\\theta A⋅B=∣A∣∣B∣cosθ また A⋅B=AxBx+AyBy \\bm{A} \\cdot \\bm{B} = A_x B_x + A_y B_y A⋅B=Ax​Bx​+Ay​By​ となるのも,AxByA_x B_yAx​By​などが直行してxxx軸に対して(もしくはy軸に対して)大きさがないからです ベクトル積は回転が関係します ベクトルrrrとベクトルFFFのベクトル積はrrrからFFFにひねった方向に回転し,それに対する右ねじの方向に向いた力です 例えばxxx, yyy, zzz軸方向に大きさ1の単位ベクトルがあるとして xxx同士の掛け算は回転しないので大きさ000のベクトル xxxとyyyの掛け算は回転するので大きさ111のzzz方向へのベクトル yyyとxxxの掛け算は回転するので大きさ111の−z-z−z方向へのベクトル 微分 偏微分 偏微分は変数を定数として見るというものです 偏微分は∂\\partial∂を使って演算をします おそらく問題ないと思います 全微分 全微分は以下のような式です Δf=∂f∂xΔx+∂f∂yΔy+∂f∂zΔz \\Delta f = \\frac{\\partial f}{\\partial x}\\Delta x + \\frac{\\partial f}{\\partial y}\\Delta y + \\frac{\\partial f}{\\partial z}\\Delta z Δf=∂x∂f​Δx+∂y∂f​Δy+∂z∂f​Δz この数式がどのようなイメージを持つかを説明するため,x−yx-yx−yの2次元に次元を落として考えます 場の量fffはx−yx-yx−y平面上で定義された量でかつスカラーとしておけばx−yx-yx−y平面上を覆うような曲面だということがわかると思います この微小な領域を考えます その微小な領域のxxx, yyy方向での変化をそれぞれΔx\\Delta xΔx, Δy\\Delta yΔyとすると,zzz軸方向の変化は∂f∂yΔy+∂f∂xΔx\\frac{\\partial f}{\\partial y}\\Delta y + \\frac{\\partial f}{\\partial x}\\Delta x∂y∂f​Δy+∂x∂f​Δx つまり,全微分とはそのような微小な領域の体積を求めているのです ∇\\nabla∇関連の公式の意味 gradψ\\psiψ(∇ψ\\nabla \\psi∇ψ) スカラー場ψ\\psiψから傾斜を求めるときに使うのがgradψgrad \\psigradψ(∇ψ\\nabla \\psi∇ψ)です これも全微分の例と同様に考えることが可能です つまり,スカラー場ψ\\psiψの最大傾斜の方向を向き,傾斜が大きければ大きいほどその値も大きなベクトルです divAdiv AdivA(∇A\\nabla A∇A) 具体的に divD=ρ div \\bm{D} = \\rho divD=ρ を考えてみます この式の両辺に微小体積ΔV\\Delta VΔVをかけるとh divDΔV=ρΔV div \\bm{D} \\Delta V = \\rho \\Delta V divDΔV=ρΔV 右辺はρ\\rhoρを電荷密度としてその体積の中にある電気量の合計です divDΔV=(∂Dx∂x+∂Dy∂y+∂Dz∂z)ΔxΔyΔz div \\bm{D} \\Delta V = \\left( \\frac{\\partial D_x}{\\partial x} + \\frac{\\partial D_y}{\\partial y} + \\frac{\\partial D_z}{\\partial z} \\right) \\Delta x \\Delta y \\Delta z divDΔV=(∂x∂Dx​​+∂y∂Dy​​+∂z∂Dz​​)ΔxΔyΔz ここで∂Dx/∂x\\partial D_x / \\partial x∂Dx​/∂xだけを考えます ∂Dx∂xΔxΔyΔz=∂Dx∂xΔx⋅ΔyΔz \\frac{\\partial D_x}{\\partial x}\\Delta x \\Delta y \\Delta z = \\frac{\\partial D_x}{\\partial x}\\Delta x \\cdot \\Delta y \\Delta z ∂x∂Dx​​ΔxΔyΔz=∂x∂Dx​​Δx⋅ΔyΔz となります.このとき∂Dx∂x\\frac{\\partial D_x}{\\partial x}∂x∂Dx​​はΔx\\Delta xΔxだけ変化したDxD_xDx​の増加分なので次のように書けます ∂Dx∂xΔx⋅ΔyΔz=Δx(x+Δx)−Δx(x)ΔyΔz \\frac{\\partial D_x}{\\partial x}\\Delta x \\cdot \\Delta y \\Delta z = { \\Delta_x (x + \\Delta x) - \\Delta_x(x)} \\Delta y \\Delta z ∂x∂Dx​​Δx⋅ΔyΔz=Δx​(x+Δx)−Δx​(x)ΔyΔz つまり,微小な面を通過する電束の差分です この値を積分すると体積全体から出てくる電束を求めることができます.よって ∫VdivDdV=∫SD⋅dS \\int_V div \\bm{D} dV = \\int_S \\bm{D} \\cdot dS ∫V​divDdV=∫S​D⋅dS という書くことができます.Δ\\DeltaΔを電束密度に限らず,任意のベクトルにするとA\\bm{A}Aと書け, ∫VdivAdV=∫SA⋅ndS \\int_V div \\bm{A} dV = \\int_S \\bm{A} \\cdot \\bm{n} dS ∫V​divAdV=∫S​A⋅ndS と書くことができます.これは体積積分とその表面の面積積分の関係を示す公式でガウスの定理と言います(電磁気の場合はガウスの法則) rotArot ArotA(∇×A\\nabla \\times A∇×A) これは回転のイメージです ここでもΔx\\Delta xΔxに関してだけ見ると計算結果は (∂Ay∂x−∂Ax∂y)ΔxΔy \\left( \\frac{\\partial A_y}{\\partial x} - \\frac{\\partial A_x}{\\partial y} \\right) \\Delta x \\Delta y (∂x∂Ay​​−∂y∂Ax​​)ΔxΔy となります.これは 回転の効果を足したもの=(Δ×A大きさ×(経路で囲まれる面積)) \\text{回転の効果を足したもの} = (\\Delta \\times \\bm{A}\\text{大きさ} \\times (\\text{経路で囲まれる面積})) 回転の効果を足したもの=(Δ×A大きさ×(経路で囲まれる面積)) という意味を持ちます.つまりこの値に対して線積分をすれば ∮CA⋅ds=∫s(Δ×A)n⋅dS \\oint_C \\bm{A} \\cdot d\\bm{s} = \\int_s (\\Delta \\times \\bm{A})_n \\cdot dS ∮C​A⋅ds=∫s​(Δ×A)n​⋅dS これがストークスの定理です ガウスの定理は体積積分と面積分の関係ですが,ストークスの定理は線積分と面積積分の関係です ラプラシアン(∇⋅(∇ψ)\\nabla \\cdot (\\nabla \\psi )∇⋅(∇ψ)) gradのdivdivdivです ∇×(∇ψ)=0\\nabla \\times (\\nabla \\psi) = 0∇×(∇ψ)=0 ∇\\nabla∇を掛けた(grad)場合,xxx成分にはyyy成分とzzz成分のみしかないので,それに∇\\nabla∇をベクトル積したもの(rot)は当然000のベクトルとなります ∇⋅(∇×A)=0\\nabla \\cdot (\\nabla \\times A) = 0∇⋅(∇×A)=0 これも上と変わりません 物理的なイメージを考えれば当然の式です "},"elec/elec02.html":{"url":"elec/elec02.html","title":"02 Coulomb & Gaussian","keywords":"","body":"02 Coulomb & Gaussian クーロンの法則 2つの点電荷の間にどのような力が働くかを示す法則です 電気量q1q_1q1​の点電荷AAAと電気量q2q_2q2​の点電荷BBBがあるとして,その距離がrrrのとき,その間に働くクーロン力(静電気力)の大きさFFFは F=kq1q2r F = k \\frac{q_1 q_2}{r} F=krq1​q2​​ ただし,k=14πε0k = \\frac{1}{4 \\pi \\varepsilon_0}k=4πε0​1​は比例定数です またAAAがBBBから受け取るクーロン力は FAB=14πε0q1q2r3r F_{\\bm{A}\\bm{B}} = \\frac{1}{4\\pi \\varepsilon_0}\\frac{q_1 q_2}{r^3}\\bm{r} FAB​=4πε0​1​r3q1​q2​​r と表せます 電場 電場とは点電荷から湧き出るエネルギーのようなものです クーロンの法則から電場は E=q4πε0r2n \\bm{E} = \\frac{q}{4 \\pi \\varepsilon_0 r^2}\\bm{n} E=4πε0​r2q​n と定義できます それを電気力線という架空の存在で表現します.わかりやすく1クーロンで1本という風に決めます このとき,あるq(>0)q(>0)q(>0)の電気量を持った点電荷からqqq本の電気力線が湧き出ています.点電荷を中心とした球面の表面積は4πr24\\pi r^24πr2なので球面上での電気力線の密度DDDは D=q4πr2 D = \\frac{q}{4 \\pi r^2} D=4πr2q​ ベクトルで表現すると(n\\bm{n}nは球の中心から外側に向かう単位ベクトル) D=q4πr2n D = \\frac{q}{4 \\pi r^2}\\bm{n} D=4πr2q​n 電気力線の密度は電束密度と呼び,電気力線のことは電束と呼びます ここで電束密度の式とクーロンの法則で定義した電場の式を比べる.すると両者の違いは1/ε01 / \\varepsilon_01/ε0​しかないことがわかります この違いは単位を揃えるための便宜的なものです ここでは電気量qqqの点電荷から生じる電気力線の本数を電束密度と電場の単位を揃えるためにq/ε0q / \\varepsilon_0q/ε0​とします ガウスの法則 点電荷を囲む球面から通って出ていく電気力線の本数は点電荷の持つ電気量(÷ε0\\div \\varepsilon_0÷ε0​)に等しいというのは直感的に理解できます この関係を式で表すのがガウスの法則です 電気力線が面を直角に通過する時はその本数は｢電場(電気力線の密度) ×\\times× 面積｣ですが,斜めに通過する可能性もあります 通過する面積は実質的にはdScos⁡θdS\\cos \\thetadScosθとなっているので,面に対する単位法線ベクトルn\\bm{n}nを用いて ∫SE⋅ndS=qε0 \\int_S \\bm{E} \\cdot \\bm{n} dS = \\frac{q}{\\varepsilon_0} ∫S​E⋅ndS=ε0​q​ と表わせ,これがガウスの法則です 左辺はガウスの定理によって ∫SE⋅ndS=∫VdivEdV \\int_S \\bm{E} \\cdot \\bm{n} dS = \\int_V div \\bm{E} dV ∫S​E⋅ndS=∫V​divEdV となります 左辺は｢電気力線の密度と面積｣から求めた電気量です.右辺は｢電気力線の湧き出しと体積｣から求めた電気量です そこで微小な領域dVdVdVで電場の式は divEdV=qε0 div \\bm{E}dV = \\frac{q}{\\varepsilon_0} divEdV=ε0​q​ 確認ですがqε0\\frac{q}{\\varepsilon_0}ε0​q​は電気力線の数です ここで,微小領域に存在する電荷の密度[C/m3][C/m^3][C/m3]を ρ=qdV \\rho = \\frac{q}{dV} ρ=dVq​ とすれば divE=ρε0 div\\bm{E} = \\frac{\\rho}{\\varepsilon_0} divE=ε0​ρ​ となります.これはマクスウェル方程式の1つです これを電解密度D\\bm{D}Dを用いて divD=ρ div \\bm{D} = \\rho divD=ρ と書くこともできます つまり今回は クーロンの法則→電場の式→ガウスの法則→divE=ρε0div\\bm{E}=\\frac{\\rho}{\\varepsilon_0}divE=ε0​ρ​ という式の変化を見ました "},"elec/elec03.html":{"url":"elec/elec03.html","title":"03 Potential","keywords":"","body":"03 Potential "},"comp-theo/comp-theo00.html":{"url":"comp-theo/comp-theo00.html","title":"Computational theory","keywords":"","body":"Computational theory "},"algo/algo00.html":{"url":"algo/algo00.html","title":"Algorithm","keywords":"","body":"Algorithm OS,暗号技術,人工知能,ネットワーク,コンパイラー...などなどあらゆる情報科学の分野で必要となる知識 Order "},"algo/algo01.html":{"url":"algo/algo01.html","title":"01 Order","keywords":"","body":"Order アルゴリズムを考える際,一つの指標となるのが計算量です 計算量とは空間計算量･時間計算量がありますがどちらも基本となる考え方は同じです ここでは時間計算量について考えます Asymptotic Analysis 計算量とはそのアルゴリズムがどのような関数に漸近するのかを考えたものです そのため,以下のようなメリット･デメリットがあります メリット 環境に依存しないアルゴリズムそのものの良し悪しについて考えられる よりアルゴリズムを取り扱いやすくする デメリット 入力が大きい時しか考慮していない この計算量を求める方法は Tree Method Master Theorem(Generalized Method) Substitution Method 以上の3種類が有ります Tree Methodを一般化したものなので以下ではMaster TheoremとSustitution Method を扱います その前に計算量の記法について学びましょう 計算量記法 ここではBig-O記法･Big-Omega記法･Big-Theta記法について考えます Big-O T(n)T(n)T(n)(計算にかかる時間で,値は正で単調増加だとします)とg(n)g(n)g(n)が正の整数の関数だとします ∃c,n0>0 s.t. ∀n≥n00≤T(n)≤cg(n) \\begin{aligned} &\\exists{c, n_0} > 0 \\text{ s.t. } \\forall{n} \\geq n_0 \\\\ &0 \\leq T(n) \\leq cg(n) \\end{aligned} ​∃c,n0​>0 s.t. ∀n≥n0​0≤T(n)≤cg(n)​ であるとき,T(n)∈O(g(n))T(n) \\in O(g(n))T(n)∈O(g(n))とします ` ある000より大きいn0n_0n0​以上のnnnでは,常にT(n)T(n)T(n)はg(n)g(n)g(n)の定数倍よりも小さいということです つまり,無限に大きいnnnでg(n)g(n)g(n)はT(n)T(n)T(n)の上限であるということです T(n)∈O(g(n))T(n) \\in O(g(n))T(n)∈O(g(n))はT(n)=O(g(n))T(n) = O(g(n))T(n)=O(g(n))とも書きます(こう書く場合の方が多いです) Big-Omega Big-O記法は上限を表現するためでしたが,Big-Omega記法は下限を表現するための記法です 上と同じ関数T(n)T(n)T(n)とg(n)g(n)g(n)を用いて ∃c,n0>0 s.t. ∀n≥n00≤cg(n)≤T(n) \\begin{aligned} &\\exists{c, n_0} > 0 \\text{ s.t. } \\forall{n} \\geq n_0 \\\\ &0 \\leq cg(n) \\leq T(n) \\end{aligned} ​∃c,n0​>0 s.t. ∀n≥n0​0≤cg(n)≤T(n)​ のときT(n)∈Ω(g(n))T(n) \\in \\Omega(g(n))T(n)∈Ω(g(n))もしくはT(n)=Ω(g(n))T(n) = \\Omega(g(n))T(n)=Ω(g(n))とします ある000より大きいn0n_0n0​以上のnnnでは,常にT(n)T(n)T(n)はg(n)g(n)g(n)の定数倍よりも大きいということです つまり,無限に大きいnnnでg(n)g(n)g(n)はT(n)T(n)T(n)の下限であるということです Big-Theta Big-Theta記法は上の記法のどちらもが当てはまるときです つまり,以下の式を満たします ∃c0,c1,n0>0 s.t. ∀n≤n00≤c0⋅g(n)≤f(n)≤c1⋅g(n) \\begin{aligned} \\exists{c_0, c_1, n_0} &> 0 \\text{ s.t. } \\forall{n} \\leq n_0 \\\\ 0 &\\leq c_0 \\cdot g(n) \\leq f(n) \\leq c_1\\cdot g(n) \\end{aligned} ∃c0​,c1​,n0​0​>0 s.t. ∀n≤n0​≤c0​⋅g(n)≤f(n)≤c1​⋅g(n)​ Big-Theta記法は無限に大きいnnnでg(n)g(n)g(n)の定数倍にT(n)T(n)T(n)が挟まれる状態にあるということです Master Theorem 最初に定理を紹介し,あとから説明をします a≥1a\\geq 1a≥1, b≥1b \\geq 1b≥1, dddをnnnに独立な定数とします このときT(n)=a⋅T(nb)+O(nd)T(n)=a \\cdot T(\\frac{n}{b}) + O(n^d)T(n)=a⋅T(bn​)+O(nd)とすると以下の式を満たします T(n)={O(ndlog(n))(a=bd)O(nd)(abd)O(nlogb(a))(a>bd) T(n) = \\begin{cases} O(n^{d}log(n)) (a = b^d) \\\\ O(n^d) (a b^d) \\end{cases} T(n)=⎩⎪⎨⎪⎧​O(ndlog(n))(a=bd)O(nd)(abd)O(nlogb​(a))(a>bd)​ この定理を分類定理(master theorem)と言います aaa : 分割された下位問題の数 bbb : 入力の大きさが縮む倍率(2→12 \\rightarrow 12→1のときはb=2b=2b=2) ddd : 全ての問題を分割し統合するのに必要な計算量 この定理は問題を分割し,それらを更に分割し解くという考えを使っています(これを再帰と言います) 入力がnnnの問題をT(n)T(n)T(n)で解くアルゴリズムを考えます 入力nnnをaaa個に分割する 分割された入力をT(nb)T(\\frac{n}{b})T(bn​)で解く 分割された各アルゴリズムに対してかかる時間が(O(nlog⁡b(a))(O(n^{\\log_{b}(a)})(O(nlogb​(a))である このようにするとT(n)T(n)T(n)は上のような式になります.実際にそれぞれのケースごとに正しいか確認してみましょう a=bda=b^da=bdの場合 T(n)=c⋅nd⋅∑t=0log⁡b(n)(abd)t=c⋅nd⋅∑t=0log⁡b(n)1=c⋅nd⋅(log⁡b(n)+1)=c⋅nd⋅(log⁡(n)log⁡(b)+1)=O(ndlog⁡(n)) \\begin{aligned} T(n) &= c\\cdot n^d \\cdot \\sum_{t=0}^{\\log_b(n)}(\\frac{a}{b^d})^t \\\\ &= c \\cdot n^d \\cdot \\sum_{t=0}^{\\log_b(n)}1 \\\\ &= c \\cdot n^d \\cdot (\\log_b(n) + 1) \\\\ &= c \\cdot n^d \\cdot (\\frac{\\log(n)}{\\log(b)} + 1) \\\\ &= O(n^d \\log(n)) \\end{aligned} T(n)​=c⋅nd⋅t=0∑logb​(n)​(bda​)t=c⋅nd⋅t=0∑logb​(n)​1=c⋅nd⋅(logb​(n)+1)=c⋅nd⋅(log(b)log(n)​+1)=O(ndlog(n))​ abda abdの場合 T(n)=c⋅nd⋅∑t=0log⁡b(n)(abd)t⏞1未満 T(n) = c\\cdot n^d \\cdot \\sum_{t=0}^{\\log_b(n)}\\overbrace{(\\frac{a}{b^d})^t}^{1\\text{未満}} T(n)=c⋅nd⋅t=0∑logb​(n)​(bda​)t​1未満​ ここで,等比級数の和は一般項xxxが1より大きい時は次数が最大の項が支配的になり,xxxが0より大きく1より小さい時は次数が最小の項が支配的になる ここでは∑t=0log⁡b(n)(abd)t\\sum_{t=0}^{\\log_b(n)}(\\frac{a}{b^d})^t∑t=0logb​(n)​(bda​)tの一般項は後者なので定数として扱える T(n)=c⋅nd⋅∑t=0log⁡b(n)(abd)t⏞1未満=c⋅nd⋅定数=O(nd) \\begin{aligned} T(n) &= c\\cdot n^d \\cdot \\sum_{t=0}^{\\log_b(n)}\\overbrace{(\\frac{a}{b^d})^t}^{1\\text{未満}} \\\\ &= c \\cdot n^d \\cdot \\text{定数} \\\\ &= O(n^d) \\end{aligned} T(n)​=c⋅nd⋅t=0∑logb​(n)​(bda​)t​1未満​=c⋅nd⋅定数=O(nd)​ a>bda > b^da>bdの場合 上の例で扱ったように一般項に注目する T(n)=c⋅nd⋅∑t=0log⁡b(n)(abd)t⏞1より大きい=O(nd(abdlogb(n)))=O(nlog⁡b(a)) \\begin{aligned} T(n) &= c\\cdot n^d \\cdot \\sum_{t=0}^{\\log_b(n)}\\overbrace{(\\frac{a}{b^d})^t}^{1\\text{より大きい}} \\\\ &=O(n^d(\\frac{a}{b^d}^{log_b(n)})) \\\\ &=O(n^{\\log_b(a)}) \\end{aligned} T(n)​=c⋅nd⋅t=0∑logb​(n)​(bda​)t​1より大きい​=O(nd(bda​logb​(n)))=O(nlogb​(a))​ Substitution Method これは以下の手順から成り立ちます 答えを推論する その推論が正しいと証明する 答えを得る 例 T(n)=2⋅T(n2)+nT(n) = 2 \\cdot T(\\frac{n}{2}) + nT(n)=2⋅T(2n​)+nという例をここでは扱います(ただしT(1)=1T(1)=1T(1)=1). 推論 この時,実際に手を動かしてみると T(n)=2⋅T(n2)+n=2⋅(2⋅T(n4)+n2)+n=4⋅T(n4)+2n=4⋅T(2⋅T(n8)+n4)+2n=8⋅T(n8)+3n=⋯ \\begin{aligned} T(n) &= 2 \\cdot T(\\frac{n}{2}) + n \\\\ &=2 \\cdot (2 \\cdot T(\\frac{n}{4}) + \\frac{n}{2}) + n \\\\ &= 4 \\cdot T(\\frac{n}{4}) + 2n \\\\ &= 4 \\cdot T(2 \\cdot T(\\frac{n}{8}) + \\frac{n}{4}) + 2n \\\\ &= 8 \\cdot T(\\frac{n}{8}) + 3n &= \\cdots \\end{aligned} T(n)​=2⋅T(2n​)+n=2⋅(2⋅T(4n​)+2n​)+n=4⋅T(4n​)+2n=4⋅T(2⋅T(8n​)+4n​)+2n=8⋅T(8n​)+3n​=⋯​ このようになり, T(n)=2t⋅T(n2t)+t⋅nT(n) = 2^t \\cdot T(\\frac{n}{2^t})+ t\\cdot nT(n)=2t⋅T(2tn​)+t⋅nになるのではないかと推測できます t=log(n)t=log(n)t=log(n)を代入し, T(n)=n⋅T(1)+log⁡(n)⋅(n)⋅n=n(log⁡(n)+1) T(n)= n \\cdot T(1) + \\log(n) \\cdot(n) \\cdot n = n(\\log(n) + 1) T(n)=n⋅T(1)+log(n)⋅(n)⋅n=n(log(n)+1) と推論できます 証明 数学的帰納法で証明します Inductive Hypothesis T(j)=j(log⁡(j)+1)T(j) = j(\\log(j)+1)T(j)=j(log(j)+1)が1≤j≤n1 \\leq j \\leq n1≤j≤nで成り立つと仮定します Base Case(n=1n=1n=1) T(1)=1=1⋅(log⁡(1)+1)T(1) = 1 = 1 \\cdot (\\log(1) + 1)T(1)=1=1⋅(log(1)+1)となり成り立ちます Inductive Step inductive hypothesisがn=k−1n=k-1n=k−1で成り立つと仮定します このとき定義からT(k)=2⋅T(k2)+kT(k)= 2 \\cdot T(\\frac{k}{2}) + kT(k)=2⋅T(2k​)+kであり,仮定よりT(k)=2⋅(k2(log⁡(k2)+1))+kT(k)= 2 \\cdot (\\frac{k}{2}(\\log(\\frac{k}{2})+1))+kT(k)=2⋅(2k​(log(2k​)+1))+kです.簡単にすると, T(k)=k(log⁡(k)+2) T(k)=k (\\log(k) + 2) T(k)=k(log(k)+2) となり,n=kn=kn=kでも成り立つことが示されました 結論 n≥1n \\geq 1n≥1 で T(n)=n(log⁡(n)+1)T(n) = n(\\log(n) + 1)T(n)=n(log(n)+1)とわかります このように事前の推論を数学的帰納法で証明することをSubstituion Methodと言います まとめ 計算量という概念を導入することでアルゴリズムの良し悪しが分かる 計算量を導く方法がある "},"comp-arch/comp-arch00.html":{"url":"comp-arch/comp-arch00.html","title":"Computer architecture","keywords":"","body":"Computer architecture コンピュータアーキテクチャとはハードウェアに関する Digital circuit "},"comp-arch/comp-arch01.html":{"url":"comp-arch/comp-arch01.html","title":"01 Digital circuit","keywords":"","body":"01 Digital circuit 抽象化 コンピュータでは一般的には連続的な値ではなく,離散的な値を扱います その理由は離散値とすることで物事を抽象化し,複雑なシステムを扱いやすくできるからです 電子計算システムでは以下のような抽象化の構造が取られています アプリケーション･ソフトウェア プログラム OS デバイス･ドライバ アーキテクチャ 命令･レジスタ マイクロアーキテクチャ データパス･コントローラ 論理 加算器･メモリ デジタル回路 ANDゲート･NOTゲート アナログ回路 増幅器･フィルタ デバイス(素子) トランジスタ･ダイオード 物理 電子 ここではこの表のうち,デジタル回路を扱います 離散 離散的とは切れ目がある値ということです デジタル回路ではブール代数という離散的な値を扱う数学を利用します ブール代数ではTRUEとFALSEの2値を扱いますが,デジタル回路では高い電圧を1=TRUE,低い電圧を0=FALSEとして扱うものが多いです.ここではそれに従います n進数 n進数は右端の桁から重みがn0,n1,n2,…n^0, n^1, n^2,\\ldotsn0,n1,n2,…となっています.これをnを基数とする表現といいます.それぞれの桁は0からn-1までの整数を表しています また,n進数の値だということを明記したい場合は1234(n)1234_{(n)}1234(n)​と右下に基数を書きます 1234(10)=1⋅103+2⋅102+3⋅101+4⋅100 1234_{(10)} = 1 \\cdot 10^3 + 2 \\cdot 10^2 + 3 \\cdot 10^1 + 4 \\cdot 10^0 1234(10)​=1⋅103+2⋅102+3⋅101+4⋅100 変換 デジタルな値は2進数なので2進数に関する変換について書きます 10進数から2進数に変換するには下のようにやります 10(10)=1⋅23+0cdot22+1⋅21+0⋅20=1010(2) \\begin{aligned} 10_{(10)} &= 1 \\cdot 2^3 + 0 cdot 2^2 + 1 \\cdot 2^1 + 0 \\cdot 2^0\\\\ &=1010_{(2)} \\end{aligned} 10(10)​​=1⋅23+0cdot22+1⋅21+0⋅20=1010(2)​​ 逆は次のようになります 10(2)=1⋅21+1⋅20=2+0=2 \\begin{aligned} 10_{(2)} &= 1 \\cdot 2^1 + 1 \\cdot 2^0\\\\ &= 2 + 0 = 2 \\end{aligned} 10(2)​​=1⋅21+1⋅20=2+0=2​ 次に16進数から2進数に変換する方法について話します. 16は2の累乗である 0~9とA,B,C,D,Eという人間でも扱いやすい数で書ける 2進数の値を短く圧縮できる という理由からよく使われます 16進数の1桁は2進数の4桁分,2進数の4桁は16進数の1桁分であるということを使って変換をします 16進数→2進数 2ED(16)=2⎵0010(2)⋅162+E⎵1111(2)⋅161+D⎵1110(2)⋅160=0010,1111,1110(2)=10,1111,1110(2) \\begin{aligned} 2ED_{(16)} &= \\underbrace{2}_{0010_{(2)}} \\cdot 16^2 + \\underbrace{E}_{1111_{(2)}} \\cdot 16^1 + \\underbrace{D}_{1110_{(2)}} \\cdot 16^0\\\\ &= 0010,1111,1110_{(2)} = 10,1111,1110_{(2)} \\end{aligned} 2ED(16)​​=0010(2)​2​​⋅162+1111(2)​E​​⋅161+1110(2)​D​​⋅160=0010,1111,1110(2)​=10,1111,1110(2)​​ 2進数→16進数 1101,0110=C6(2) \\begin{aligned} 1101,0110 = C6_{(2)} \\end{aligned} 1101,0110=C6(2)​​ 単位 1か0を表せるような値の単位をビットと言います 8ビットで1バイト,4ビットで1ニブルです マイクロプロセッサはデータをワードというかたまりで処理し,nビットのワードを処理するプロセッサをnビットプロセッサと言います 複数ビットの中で最も小さい値を表現するビットを最上位値ビット(most significant bit, msb),最も小さい値を表現するビットを最下位値ビット(least significant bit, lsb)と呼びます 210≈1032^10 \\approx 10^3210≈103をキロ,220≈1062^20 \\approx 10^6220≈106をメガ,230≈1092^30 \\approx 10^9230≈109をギガと言います 繰り上がりのことをキャリと言い,繰り上げされるビットのことをキャリビットと言います 加算の結果,使えない桁にキャリビットが入ることをオーバーフロー(桁溢れ)と言います 符号 今までは符号(プラスマイナス)を考えていませんでした.符号を考える時,msbを符号化ビットとして使います.msbが1なら負,0なら正とします. N桁の値だとして,残りのN-1桁で値を表現します しかしこのままだと負の値と正の加算した際に値が正確に計算されません.そのために2の補数を導入します これは｢符号ビット以外のビットを反転させて1を足すことでプラスマイナスを反転させた数｣のことです 11(10)=01011(2)11_{(10)} = 01011_{(2)}11(10)​=01011(2)​の2の補数は−11=10100(2)+1(1)=10101(2)-11=10100_{(2)} + 1_{(1)} = 10101_{(2)}−11=10100(2)​+1(1)​=10101(2)​となります これによりNビットで[−2N−1,2N−1−1][-2^{N-1}, 2^{N-1}-1][−2N−1,2N−1−1]の整数を表現できます.N+1桁目へのキャリは無視するのでオーバーフローではありませんが,同じ正負の和により符号が変わるのはオーバーフローです \\[ \\mu = \\frac{1}{N} \\sum_{i=0} x_i \\] "},"signal/signal00.html":{"url":"signal/signal00.html","title":"Signal processing","keywords":"","body":"Signal processing 信号とは時間,空間に伴って変化する情報のことです.情報を扱う学問には欠かせません Sine wave & Linear system Complex plane "},"signal/signal01.html":{"url":"signal/signal01.html","title":"01 Sine wave & Linear system","keywords":"","body":"01 Sine wave & Linear system 信号 時間と共に変化する波形を時間信号と言います 時間信号はその特性によって分類することが可能です.以下の図でそれを書きました 信号名 特性 不規則信号 or 確率的信号 不規則に確率的に変動する 確定信号 確率的に変動しない 周期信号 周期的に同じ波形がくり返される 非周期信号 周期を持たない 連続時間信号 連続的な時間で記せる 離散時間信号 離散的な信号値だけを扱う システム 伝送システムとは入力信号x(t)x(t)x(t)を受取,出力信号y(t)y(t)y(t)を返すシステムという抽象的なものを扱う システムは以下のように分類できる システム名 特性 線形システム 線形である(後述) 非線形システム 非線形である 連続時間システム 入出力信号が連続時間信号 離散時間システム 入出力信号が離散信号 時変システム システムの特性が時間的に変化する 時不変システム システムの特性が時間的に固定されている 正弦波 時間信号は x(t)=Asin⁡ωtx(t)=Acos⁡ωt \\begin{aligned} x(t) &= A\\sin \\omega t\\\\ x(t) &= A\\cos \\omega t \\end{aligned} x(t)x(t)​=Asinωt=Acosωt​ という式で表されます.前者が正弦波,後者が余弦波です.位相をずらせば同じ信号なので,一般化したcos⁡\\coscosで表現しても正弦波信号と呼びます これは次のように表せます x(t)=Acos⁡(ω+θ) x(t) = A \\cos (\\omega + \\theta) x(t)=Acos(ω+θ) 周期 正弦波信号は円運動の見かけの姿と考えることもできます 正弦波が同じ波形を繰り返す最小の時間を基本周期と言い,TTTで表します(単位はsss) このとき1秒間での1周期分の波形の数はf=1/Tf=1/Tf=1/Tとなります.このfffを周波数と言います 三角関数は2π2\\pi2πずれると同じ値になるのでωT=2π\\omega T = 2 \\piωT=2πです.つまり ω=2πTω=2πf \\omega = \\frac{2\\pi}{T} \\omega = 2 \\pi f ω=T2π​ω=2πf よって時間信号は次のように表せます x(t)=Acos⁡(2πft+θ) x(t) = A \\cos (2\\pi f t + \\theta) x(t)=Acos(2πft+θ) その他性質 加法定理を使って cos⁡(α+β)=cos⁡αcos⁡β−sin⁡αsin⁡β \\cos (\\alpha + \\beta) = \\cos \\alpha \\cos \\beta - \\sin \\alpha \\sin \\beta cos(α+β)=cosαcosβ−sinαsinβ これより上の式から以下のように書けます x(t)=Acos⁡(ωt+θ)=acos⁡ωt+bsin⁡ωt x(t)=A\\cos(\\omega t + \\theta)=a \\cos \\omega t + b \\sin \\omega t x(t)=Acos(ωt+θ)=acosωt+bsinωt ただし,a=Acos⁡θa=A \\cos \\thetaa=Acosθ, b=−Asin⁡θb=-A \\sin \\thetab=−Asinθ 線形システム 線形とは以下のようなことを言います 2つの信号の和が入力された時,その出力はそれぞれの出力の和 x1(t)→y1(t),x2(t)→y2(t)x_1(t) \\to y_1(t), x_2(t) \\to y_2(t)x1​(t)→y1​(t),x2​(t)→y2​(t) のとき x1(t)+x2(t)→y1(t)+y2(t) x_1(t) + x_2(t) \\to y_1(t) + y_2(t) x1​(t)+x2​(t)→y1​(t)+y2​(t) 入力を定数倍すると出力も定数倍になる ax(t)→ay(t) ax(t) \\to ay(t) ax(t)→ay(t) また次のように数式で表現することもできます x(t)=∑kakxk(t)y(t)=ϕ[x(t)]=∑kakϕ[xk(t)] \\begin{aligned} x(t) &= \\sum_k a_k x_k (t)\\\\ y(t) &= \\phi [x(t)] = \\sum_k a_k \\phi [x_k(t)] \\end{aligned} x(t)y(t)​=k∑​ak​xk​(t)=ϕ[x(t)]=k∑​ak​ϕ[xk​(t)]​ 線形システムに制限は信号を入力すると,出力も同じ周波数の正弦波信号となります(これは線形システムの定義から明らかです) そして,入出力をそれぞれ x(t)=A1cos⁡(ωt+θ1)=A1cos⁡(2πft+θ1)y(t)=A2cos⁡(ωt+θ2)=A2cos⁡(2πft+θ2) \\begin{aligned} x(t)&=A_1 \\cos(\\omega t + \\theta_1) = A_1 \\cos (2\\pi f t + \\theta_1)\\\\ y(t)&=A_2 \\cos(\\omega t + \\theta_2) = A_2 \\cos (2\\pi f t + \\theta_2)\\\\ \\end{aligned} x(t)y(t)​=A1​cos(ωt+θ1​)=A1​cos(2πft+θ1​)=A2​cos(ωt+θ2​)=A2​cos(2πft+θ2​)​ とおくと,変化量は 振幅についてはA2/A1A_2/A_1A2​/A1​ 位相についてはθ2−θ1\\theta_2-\\theta_1θ2​−θ1​ となります.これらを周波数fffの関数として表し,それぞれ振幅伝達特性,位相伝達特性と呼びます.そして,線形システムの制限は応答はその振幅伝達特性と位相伝達特性を周波数の関数として表示することによって記述されます 線形システムの基本入力信号としてインパルス信号と呼ばれる特殊な信号があります インパルス信号とはt=0t=0t=0のときのみ値が∞\\infty∞で,それ以外では000となり,波形の面積が111であるような信号です.記号ではδ(t)\\delta(t)δ(t)として,δ信号と呼ぶことも有ります δ(t)={∞(t=0)0(t≠0) \\begin{aligned} \\delta(t)= \\begin{cases} \\infty & (t = 0)\\\\ 0 &(t \\ne 0)& \\end{cases} \\end{aligned} δ(t)={∞0​(t=0)(t≠0)​​​ ∫ϵϵδ(t)dt=1 \\int^{\\epsilon}_{\\epsilon}\\delta(t)dt=1 ∫ϵϵ​δ(t)dt=1 インパルス信号には次のような性質が有ります x(t0)x(t_0)x(t0​)がt=t0t=t_0t=t0​で連続なとき ∫−∞∞δ(t−t0)x(t)dt=x(t0) \\int^{\\infty}_{-\\infty}\\delta(t-t_0)x(t)dt=x(t_0) ∫−∞∞​δ(t−t0​)x(t)dt=x(t0​) です.実はこの式を満たす関数がδ\\deltaδ関数だと定義しているのです インパルス応答 δ(t)\\delta(t)δ(t)を入力としたときの出力応答h(t)h(t)h(t)をインパルス応答と言います δ(t−τ)→h(t−τ) \\delta(t - \\tau) \\to h(t - \\tau) δ(t−τ)→h(t−τ) という式が時間τ\\tauτによらず成立するとき,このシステムは時不変性を持つとい言います 次の式をx(t)x(t)x(t)とh(t)h(t)h(t)の畳み込み積分と言います y(t)=∫−∞∞h(τ)x(t−τ)dτ y(t) = \\int^{\\infty}_{-\\infty}h(\\tau)x(t - \\tau)d\\tau y(t)=∫−∞∞​h(τ)x(t−τ)dτ これは y(t)=h(t)⊗x(t) y(t) = h(t) \\otimes x(t) y(t)=h(t)⊗x(t) とも表せます.この式は線形システムにおいて,入力x(t)x(t)x(t)とインパルス応答h(t)h(t)h(t)が与えられればその畳込み積分によって出力y(t)y(t)y(t)が計算できるということを示しています つまり ｢x(t)x(t)x(t)から直接y(t)y(t)y(t)を求める｣ではなく ｢x(t)x(t)x(t)とδ(t)\\delta(t)δ(t)から求めた出力h(t)h(t)h(t)からy(t)y(t)y(t)を求める｣ で済むということです フーリエ級数展開入門 周期TTTの信号x(t)x(t)x(t)を考えます.これを角周波数ω0=2π/T\\omega_0 = 2\\pi / Tω0​=2π/Tの整数倍の角周波数nω0n \\omega_0nω0​の正弦波の線形合成 x(t)=∑n=0∞Ancos⁡(nωt+θn) x(t) = \\sum^{\\infty}_{n=0}A_n \\cos(n \\omega t + \\theta_n) x(t)=n=0∑∞​An​cos(nωt+θn​) で表します この式の右辺の制限は信号をsin信号とcos信号の和に分解して表現,つまりフーリエ級数展開すると以下の式になります x(t)=a02+∑n=1∞(ancos⁡nω0t+bnsin⁡nω0t) x(t) = \\frac{a_0}{2}+ \\sum^{\\infty}_{n=1}(a_n \\cos n \\omega_0 t + b_n \\sin n \\omega_0 t) x(t)=2a0​​+n=1∑∞​(an​cosnω0​t+bn​sinnω0​t) ただし,ω0=2πT\\omega_0 = \\frac{2\\pi}{T}ω0​=T2π​です このとき,係数は次のようになります an=2T∫T/2T/2x(t)cos⁡nω0tdtbn=2T∫T/2T/2x(t)sin⁡nω0tdt \\begin{aligned} a_n &= \\frac{2}{T}\\int^{T/2}_{T/2}x(t)\\cos n \\omega_0 t dt\\\\ b_n &= \\frac{2}{T}\\int^{T/2}_{T/2}x(t)\\sin n \\omega_0 t dt \\end{aligned} an​bn​​=T2​∫T/2T/2​x(t)cosnω0​tdt=T2​∫T/2T/2​x(t)sinnω0​tdt​ 信号x(t)x(t)x(t)は偶関数のとき式は定数項とcosの項だけとなり,奇関数のとき式は定数項とsinの項だけとなります.それぞれフーリエ余弦級数,フーリエ正弦級数と呼ばれています 信号x(t)x(t)x(t)が周期的でない場合はフーリエ変換という手法が用いられます "},"signal/signal02.html":{"url":"signal/signal02.html","title":"02 Complex plane","keywords":"","body":"02 Complex plane 信号とシステムを実数ではなく複素数で表現すると扱いが容易になります.ここではその導入として複素数と複素平面についての基礎事項をおさらいします 複素数 定義 実数xxxとyyyが与えられた時に,虚数単位jjjを付けて z=x+jy z = x + jy z=x+jy とおいたものを複素数(complex number)といい,xxxを実部(real part),yyyを虚部(imaginary part)と言います.それぞれ x=Rezy=Imz \\begin{aligned} x &= Re z\\\\ y &= Im z \\end{aligned} xy​=Rez=Imz​ と記します.実部がなく虚部だけの複素数を純虚数と言います 複素数zzzにおいて,虚部の符号を変えたもの z∗=x−jy z^{\\ast} = x - jy z∗=x−jy をzzzの複素共役(complex conjugate)もしくは共役複素数と言います.zzzと複素共役z∗z^{\\ast}z∗の和と積はどちらも実数となります 複素平面 複素数をxxxを横座標,yyyを経て座標とする二次元平面上の1つの点として表示することができ,この平面のことを複素平面(complex plane)もしくはガウス平面(Gaussian plane)と言います 原点からzzzの距離rrrはピタゴラスの定理より r=x2+y2=zz∗ r = \\sqrt{x^2 + y^2} = \\sqrt{z z^{\\ast}} r=x2+y2​=zz∗​ となります.これを複素数zzzの絶対値と呼び,∣z∣|z|∣z∣と記します.また,zzzと原点を結ぶ直線が横軸の間となす角ϕ\\phiϕはzzzの偏角と呼びarg(z)arg(z)arg(z)と書きます.rrrとϕ\\phiϕを用いるとzzzの実部と虚部はそれぞれ x=rcos⁡ϕy=rsin⁡ϕ \\begin{aligned} x &= r \\cos \\phi\\\\ y &= r \\sin \\phi \\end{aligned} xy​=rcosϕ=rsinϕ​ と表せます.これとzzzの定義から z=rcos⁡ϕ+jrsin⁡ϕ=r(cos⁡ϕ+jsin⁡ϕ) \\begin{aligned} z &= r\\cos \\phi + jr \\sin \\phi\\\\ &= r(\\cos \\phi + j \\sin \\phi) \\end{aligned} z​=rcosϕ+jrsinϕ=r(cosϕ+jsinϕ)​ を得ます.このような表現を極形式(polar form)と言います オイラーの公式 極形式を更に簡潔な表現とするため,オイラーの公式(Euler's formula)を用います ejϕ=cosϕ+jsinϕ e^{j \\phi} = cos \\phi + j sin \\phi ejϕ=cosϕ+jsinϕ オイラーの公式を使うと極形式は次のように表せます z=r(cos⁡ϕ+jsin⁡ϕ)=rejϕ z = r(\\cos \\phi + j \\sin \\phi) = re^{j \\phi} z=r(cosϕ+jsinϕ)=rejϕ 複素正弦波信号 円運動と複素数の関連 円上にある絶対値AAAの点Pを考えます.その偏角ϕ\\phiϕを時間に比例してϕ=ωt\\phi = \\omega tϕ=ωtで変化させます.このとき,点Pの実部と虚部の時間変化は x(t)=Acos⁡ωty(t)=Asin⁡ωt \\begin{aligned} x(t) &= A \\cos \\omega t\\\\ y(t) &= A \\sin \\omega t \\end{aligned} x(t)y(t)​=Acosωt=Asinωt​ となります.これはそれぞれ正弦波信号となっています 信号の定義 点Pに対応する複素数zzzそのものを複素数値の信号として新たに定義します これをz(t)z(t)z(t)とすれば z(t)=x(t)+y(t)=A(cos⁡ωt+jsin⁡ωt) \\begin{aligned} z(t) &= x(t) + y(t)\\\\ &= A(\\cos \\omega t + j \\sin \\omega t) \\end{aligned} z(t)​=x(t)+y(t)=A(cosωt+jsinωt)​ この右辺にオイラーの公式にϕ=ωt\\phi = \\omega tϕ=ωtを代入したejωt=cos⁡ωt+jsin⁡ωte^{j \\omega t} = \\cos \\omega t + j \\sin \\omega tejωt=cosωt+jsinωtを適用し,次のように複素正弦波信号が定義されます z(t)=Aejωt z(t) = A e^{j \\omega t} z(t)=Aejωt ただし,振幅AAA,各周波数ω\\omegaωです.ω\\omegaωが正のときは複素平面上を半時計回りに円運動する信号です 各周波数は−∞ω∞-\\infty −∞ω∞,つまり全ての実数の範囲で定義されます 一般の複素正弦波信号 振幅AAA,位相θ\\thetaθを持つ一般の複素正弦波信号は x(t)=Aej(ωt+θ) x(t) = Ae^{j(\\omega t + \\theta)} x(t)=Aej(ωt+θ) で定義されます.これは指数関数の性質を使って x(t)=A˙ejωt x(t) = \\dot{A}e^{j \\omega t} x(t)=A˙ejωt とも表現できます.これは係数AejθAe^{j\\theta}Aejθと時間信号ejωte^{j \\omega t}ejωtの積になります.係数は時間信号の振幅とみなすことができ,これを複素振幅と呼びます 複素振幅は複素数で,絶対値は元々の信号の振幅AAA,偏角が位相θ\\thetaθです 実数の正弦波信号の複素正弦波信号による合成 ejωt=cos⁡ωt+jsin⁡ωte−jωt=cos⁡ωt−jsin⁡ωt \\begin{aligned} e^{j\\omega t} &= \\cos \\omega t + j \\sin \\omega t e^{-j\\omega t} &= \\cos \\omega t - j \\sin \\omega t \\end{aligned} ejωt​=cosωt+jsinωte−jωt​=cosωt−jsinωt​ であるから,和と差を取って222で割ると cos⁡ωt=12(ejωt+e−jωt)sin⁡ωt=12j(ejωt−e−jωt) \\begin{aligned} \\cos \\omega t &= \\frac{1}{2}(e^{j \\omega t} + e^{-j \\omega t})\\\\ \\sin \\omega t &= \\frac{1}{2j}(e^{j \\omega t} - e^{-j \\omega t}) \\end{aligned} cosωtsinωt​=21​(ejωt+e−jωt)=2j1​(ejωt−e−jωt)​ となります.正弦波信号は2つの複素正弦波信号の合成で表すことができます.より一般的に次式が成り立ちます Acos⁡(ωt+θ)=A2ejθ⋅ejωt+A2ejθ⋅e−jωt A \\cos (\\omega t + \\theta) = \\frac{A}{2}e^{j \\theta} \\cdot e^{j \\omega t} + \\frac{A}{2}e^{j \\theta} \\cdot e^{-j \\omega t} Acos(ωt+θ)=2A​ejθ⋅ejωt+2A​ejθ⋅e−jωt 複素伝達関数 線形システムで複素正弦波信号を入力した場合,出力も同じ形の信号と成ります.このようにシステムに対して入出力の形が同じ信号をシステムの固有信号と言い,複素正弦波信号は線形システムの固有信号です.よって入力,出力をそれぞれx(t)x(t)x(t), y(t)y(t)y(t)とおいて x(t)=A1ejθ1ejωty(t)=A2ejθ2ejωt \\begin{aligned} x(t) &= A_1 e^{j \\theta_1}e^{j \\omega t}\\\\ y(t) &= A_2 e^{j \\theta_2}e^{j \\omega t} \\end{aligned} x(t)y(t)​=A1​ejθ1​ejωt=A2​ejθ2​ejωt​ とおいて,両者の比を取ると 出力入力=A2A1ej(θ2−θ1) \\frac{\\text{出力}}{\\text{入力}} = \\frac{A_2}{A_1}e^{j(\\theta_2 - \\theta_1)} 入力出力​=A1​A2​​ej(θ2​−θ1​) これをjωj\\omegajωの関数として H(jω)=∣H(jω)∣ej∠H(jω) H(j\\omega) = |H(j\\omega)|e^{j \\angle H(j \\omega)} H(jω)=∣H(jω)∣ej∠H(jω) と表し,線形システムの複素伝達関数もしくは伝達関数(transfer functoin)と呼びます.その振幅伝達特性∣H(jω)∣n|H(j \\omega)|n∣H(jω)∣n,位相伝達特性∠H(jω)\\angle H(j \\omega)∠H(jω)はそれぞれ ∣H(jω)∣=A2A1∠H(jω)=θ2−θ1 |H(j \\omega)| = \\frac{A_2}{A_1} \\angle H(j \\omega) = \\theta_2 - \\theta_1 ∣H(jω)∣=A1​A2​​∠H(jω)=θ2​−θ1​ となります "},"clang/clang00.html":{"url":"clang/clang00.html","title":"C language","keywords":"","body":"C language "},"clang/clang01.html":{"url":"clang/clang01.html","title":"01 Introduction to C","keywords":"","body":"01 Introduction to C Hello Worldとは ここではC言語に入門するために最低限の知識を書きます C言語には聖典とされている(時代遅れな)本があり,その名も\"The C Programming Language\"という本です 著者であるBrian KernighanとDennis Ritchieから\"K&R\"と呼ばれています この本で最初に\"hello world\"を出力させることから入門書では最初にこの文字列を出力させることが慣習となっています ここでも｢では出力させましょう｣と行きたい所ですが,その前に環境構築をします 環境構築 環境構築とは簡単に言えばPCの設定です と言ってもPCにも色々あるので｢ gcc インストール｣(には\"Windows\", \"Ubuntu\", \"Mac\"などが入ると思います)とGoogleで検索すると記事がヒットするのでその記事に従いましょう エディタに関してはこのサイト内に記事があるのでそれを読んでくれると助かります 入門 Hello World "},"info-theo/info-theo00.html":{"url":"info-theo/info-theo00.html","title":"Information theory","keywords":"","body":"Information theory 情報理論とは情報を数量的に捉える理論のことです Entropy Information Resource Marcov chain "},"info-theo/info-theo01.html":{"url":"info-theo/info-theo01.html","title":"01 Entropy","keywords":"","body":"01 Entropy 情報を知識を｢不確実｣なものから｢確実｣なものへとするものと定義します その際 情報量=不確実さ \\text{情報量} = \\text{不確実さ} 情報量=不確実さ と式で表せます 事象がnnn個あり,その中から1≤i≤n1 \\leq i \\leq n1≤i≤nとしてAiA_iAi​が起きるのを知らせる情報量を求める関数をf(n)f(n)f(n)とします nnn個が大きいほど,どの事象が起こったのか見当がつきにくいのでf(n)f(n)f(n)は大きくなります また,｢nnn個の事象をkkk個ずつmmm組に分けて,そのmmm組の中から1つを選ぶ,その組のkkk個の中から1つを選ぶ｣場合と｢直接n(=m+k)n(=m+k)n(=m+k)個の事象から1つを選ぶ｣場合で得られる情報量は同じです. つまり f(mk)=f(m)+f(k) f(mk) = f(m) + f(k) f(mk)=f(m)+f(k) が成り立ちます これを情報の加法性と言います ビット ここでf(xy)=f(x)+f(y)f(xy)=f(x)+f(y)f(xy)=f(x)+f(y)とすると f(x+εx)=f{(1+ε)x}=f(1+ε)+f(x) \\begin{aligned} f(x + \\varepsilon x) &= f\\{(1 + \\varepsilon)x\\}\\\\ & = f(1 + \\varepsilon) + f(x) \\end{aligned} f(x+εx)​=f{(1+ε)x}=f(1+ε)+f(x)​ よって f(x+εx)−f(x)εx=f(1+ε)εx \\frac{f(x+\\varepsilon x) - f(x)}{\\varepsilon x} = \\frac{f(1+\\varepsilon)}{\\varepsilon x} εxf(x+εx)−f(x)​=εxf(1+ε)​ この式を(1)とします ε→0\\varepsilon \\to 0ε→0とすると左辺は明らかにf(x)f(x)f(x)の導関数f′(x)f'(x)f′(x)であり, lim⁡ε→0f(1+ε)ε=c \\lim_{\\varepsilon \\to 0} \\frac{f(1 + \\varepsilon)}{\\varepsilon} = c ε→0lim​εf(1+ε)​=c とするとこれを変形して lim⁡ε→0f(1+ε)εx=cx \\lim_{\\varepsilon \\to 0} \\frac{f(1 + \\varepsilon)}{\\varepsilon x} = \\frac{c}{x} ε→0lim​εxf(1+ε)​=xc​ であり(1)(1)(1)から左辺はf′(x)f'(x)f′(x)だから f′(x)=cx f'(x) = \\frac{c}{x} f′(x)=xc​ という微分方程式を得る.両辺をxxxで積分して f(x)=clog⁡(x)+d(log⁡=log⁡e) f(x) = c\\log(x) + d \\;\\;\\; (\\log = \\log_e) f(x)=clog(x)+d(log=loge​) となる n=1n=1n=1のときf(1)=0f(1)=0f(1)=0であるからd=0d=0d=0 YES or Noで決まるような二者択一の情報量の単位を1ビット(bit, binary digit)とすると f(2)=1bit f(2) = 1\\text{bit} f(2)=1bit よってf(x)=clog⁡(x)f(x)=c\\log(x)f(x)=clog(x)に代入して f(2)=clog⁡e2=1 f(2) = c\\log_e 2 = 1 f(2)=cloge​2=1 つまり c⋅log⁡22log⁡2e=1 c \\cdot \\frac{\\log_2 2}{\\log_2 e} = 1 c⋅log2​elog2​2​=1 よって c=log⁡2e c = \\log_2 e c=log2​e 以上よりf(x)=log⁡2x bitf(x) =\\log_2 x \\text{ bit}f(x)=log2​x bit また以降対数の底は222とする 一般化 上の話の一般化をする ある事象AAAが起きる確率をpppとします.このときpppを有理数としてp=knp=\\frac{k}{n}p=nk​とする AAAはnnn個の等確率で起きる事象のうちkkk個をひとまとめにしたものとする.このとき,AAAが起こるのはkkk個のうちどれか1個が起こることだとすると確かにpppは上のような確率となる nnn個のうちどれかが起こったことを知るための情報量はlog⁡n\\log nlognである AAAが起こったことを教える情報量をIIIとすると,IIIだけではkkkのうちどれが起こったかはわからないから,更にlog⁡k\\log klogkの情報量が必要となる つまり情報量IIIは log⁡n=I+log⁡kI=log⁡n−log⁡k=−(log⁡k−log⁡n)=−log⁡kn \\begin{aligned} \\log n &= I + \\log k\\\\ I &= \\log n - \\log k\\\\ &= -(\\log k - \\log n)\\\\ &= - \\log \\frac{k}{n} \\end{aligned} lognI​=I+logk=logn−logk=−(logk−logn)=−lognk​​ となります また,A1,A2,..,AnA_1,A_2,..,A_nA1​,A2​,..,An​は全て確率1n\\frac{1}{n}n1​で起きるので,このうちどれかが起こったのか知らせる情報量は −log⁡1n=log⁡n -\\log \\frac{1}{n} = \\log n −logn1​=logn 以上より 定義: 確率pppの事象が実際に起こったことを知らせる情報に含まれる情報量は −log⁡2p ビット -\\log_2 p \\text{ ビット} −log2​p ビット これはpppが小さいほど大きくなります.起きにくいことが起きると知った時の方が確かにおどろくという実態とマッチしているように感じます エントロピー 情報量は起こったことを知らせる値ですが,エントロピーは不確定な状況を確定するのに必要な情報量の値です 具体的にはA1,A2,...,AnA_1,A_2,..., A_nA1​,A2​,...,An​のnnn個の事象の確率がp1,p2,..,pnp_1,p_2,..,p_np1​,p2​,..,pn​だとして∑i=1npi=1\\sum_{i=1}^{n}p_i=1∑i=1n​pi​=1であり,得られる情報量の期待値IIIは以下の通りです I=−∑i=1npilog⁡pi I = - \\sum_{i=1}^{n}p_i \\log p_i I=−i=1∑n​pi​logpi​ この値はエントロピーです 定義: nnn個の事象の確率がそれぞれp1,p2,...,pnp_1,p_2,...,p_np1​,p2​,...,pn​で発生する時どれが発生したかの不確定度(エントロピー)は H(p1,p2,...,pn)=−∑i=1npilog⁡pi H(p_1,p_2,...,p_n)=-\\sum_{i=1}^{n}p_i \\log p_i H(p1​,p2​,...,pn​)=−i=1∑n​pi​logpi​ エントロピーは以下の性質を持ちます エントロピーは非負 nnn個の事象を表すエントロピーの最大値H(n)H(n)H(n)はH(n)=log⁡nH(n)=\\log nH(n)=lognで,全ての事象が等しい確率で起こるときの不確定度 情報を得て情報エントロピーがHHHからH′H'H′に変わったときにこの情報量をI=H−H′I=H-H'I=H−H′とする 複合事象のエントロピー 複合事象とは複数の事象が組み合わさったものです 例えば事象AiA_iAi​とBjB_jBj​の複合事象の不確定度を示すエントロピーH(A,B)H(A,B)H(A,B)は H(A,B)=−∑i,jp(Ai,Bj)log⁡p(Ai,Bj) H(A, B) = - \\sum_{i,j}p(A_i, B_j)\\log p(A_i, B_j) H(A,B)=−i,j∑​p(Ai​,Bj​)logp(Ai​,Bj​) 条件付きエントロピー {同時確率分布: p(Ai,Bj)周辺確率分布: p(Ai),p(Bj)条件付き確率分布: pAi(Bj),pBj(Ai) \\begin{cases} \\text{同時確率分布: }p(A_i, B_j)\\\\ \\text{周辺確率分布: }p(A_i), p(B_j)\\\\ \\text{条件付き確率分布: }p_{A_i}(B_j), p_{B_j}(A_i) \\end{cases} ⎩⎪⎨⎪⎧​同時確率分布: p(Ai​,Bj​)周辺確率分布: p(Ai​),p(Bj​)条件付き確率分布: pAi​​(Bj​),pBj​​(Ai​)​ 確率分布は上のように分類できます 条件付き確率分布は例えばpAi(Bj)p_{A_i}(B_j)pAi​​(Bj​)はA=AiA=A_iA=Ai​のときのB=BjB= B_jB=Bj​となる確率のことです これらの確率分布は以下のような特徴を持ちます 1 ∑i,jp(Ai,Bj=1∑ip(Ai)=∑jp(Bj)=1∑jpAi(Bj)=∑ipBj(Ai)=1 \\begin{aligned} &\\sum_{i,j}p(A_i,B_j=1\\\\ &\\sum_{i}p(A_i)=\\sum_{j}p(B_j)=1\\\\ &\\sum_{j}p_{A_i}(B_j)=\\sum_{i}p_{B_j}(A_i)=1 \\end{aligned} ​i,j∑​p(Ai​,Bj​=1i∑​p(Ai​)=j∑​p(Bj​)=1j∑​pAi​​(Bj​)=i∑​pBj​​(Ai​)=1​ すべての確率を足したら111になるということです 2 p(Ai)=∑jp(Ai,Bj)p(Bj)=∑ip(Ai,Bj)\\begin{aligned} p(A_i)&=\\sum_{j}p(A_i,B_j)\\\\ p(B_j)&=\\sum_{i}p(A_i,B_j)\\\\ \\end{aligned} p(Ai​)p(Bj​)​=j∑​p(Ai​,Bj​)=i∑​p(Ai​,Bj​)​ 同時確率分布で片方を固定した場合の和は固定された方の周辺確率分布になるということです *3 p(Ai,Bj)=p(Ai)pAi(Bj)=p(Bj)pBj(Ai) p(A_i,B_j)=p(A_i)p_{A_i}(B_j)=p(B_j)p_{B_j}(A_i) p(Ai​,Bj​)=p(Ai​)pAi​​(Bj​)=p(Bj​)pBj​​(Ai​) Ai,BjA_i, B_jAi​,Bj​の同時確率分布はAiA_iAi​の確率とAiA_iAi​が起きたときのBjB_jBj​の確率を書けたものです 4 pAi(Bj)=p(Bj)pBj(Ai)p(Ai)=p(Ai,Bj)p(Ai)pBj(Ai)=p(Ai)pAi(Bj)p(Bj)=p(Ai,Bj)p(Bj) \\begin{aligned} p_{A_i}(B_j)&=\\frac{p(B_j)p_{B_j}(A_i)}{p(A_i)}=\\frac{p(A_i,B_j)}{p(A_i)}\\\\ p_{B_j}(A_i)&=\\frac{p(A_i)p_{A_i}(B_j)}{p(B_j)}=\\frac{p(A_i,B_j)}{p(B_j)} \\end{aligned} pAi​​(Bj​)pBj​​(Ai​)​=p(Ai​)p(Bj​)pBj​​(Ai​)​=p(Ai​)p(Ai​,Bj​)​=p(Bj​)p(Ai​)pAi​​(Bj​)​=p(Bj​)p(Ai​,Bj​)​​ これは条件付き確率の定義です.3から容易に求めることができます Bayesの公式とも言われています 3, 4はAi,BjA_i,B_jAi​,Bj​が独立の時に成り立ちます 次にAAAが何であるかを知ったときのBBBが何であるかについての不確定度を求めます AAAがAiA_iAi​であるとわかったときの不確定度を表すエントロピーは HAi(B)=−∑j=1mpA(Bj)log⁡pA(Bj) H_{A_i}(B)=-\\sum_{j=1}^m p_{A}(B_j)\\log p_{A}(B_j) HAi​​(B)=−j=1∑m​pA​(Bj​)logpA​(Bj​) 求めるのはAAAが何であるかは不確定であり,求めるエントロピーは全てのAiA_iAi​について平均したものであるから HA(B)=∑i=1np(Ai)HAi(B)=−∑i,jp(Ai)pAi(Bj)log⁡pAi(Bj) \\begin{aligned} H_A(B)&=\\sum_{i=1}^{n}p(A_i)H_{A_i}(B)\\\\ &=-\\sum_{i,j}p(A_i)p_{A_i}(B_j)\\log p_{A_i}(B_j) \\end{aligned} HA​(B)​=i=1∑n​p(Ai​)HAi​​(B)=−i,j∑​p(Ai​)pAi​​(Bj​)logpAi​​(Bj​)​ このエントロピーの平均値HA(B)H_A(B)HA​(B)を条件付きエントロピーと言います 条件付きエントロピーもいくつかの性質を持ちます 1 H(A,B)=H(A)+HA(B)=H(B)+HB(A)HA(B)=H(A,B)−H(A) \\begin{aligned} H(A,B)&=H(A)+H_A(B)\\\\ &=H(B)+H_B(A)\\\\ H_A(B)&=H(A,B)-H(A) \\end{aligned} H(A,B)HA​(B)​=H(A)+HA​(B)=H(B)+HB​(A)=H(A,B)−H(A)​ 最終的に同じことが分かっていれば同じ情報量になるということです 2 HA(B)≤0 H_A(B) \\leq 0 HA​(B)≤0 エントロピーは非負であるので,その平均値も非負です 等号が成立するのはHAi(B)H_{A_i}(B)HAi​​(B)が全て000になるときです 3 H(A)+H(B)≤H(A,B) H(A)+H(B) \\leq H(A,B) H(A)+H(B)≤H(A,B) AAAとBBBが独立であるときに統合は成立します 4 H(A)≥HB(A)H(B)≥HA(B) \\begin{aligned} H(A) &\\geq H_B(A)\\\\ H(B) &\\geq H_A(B) \\end{aligned} H(A)H(B)​≥HB​(A)≥HA​(B)​ これは情報の不確かさが何か情報が与えられて増える 5 H(A,B)=H(A)+HA(B)=H(B)+HB(A)HA(B)=H(A,B)−H(A) \\begin{aligned} H(A,B)&=H(A)+H_A(B)\\\\ &=H(B)+H_B(A)\\\\ H_A(B)&=H(A,B)-H(A) \\end{aligned} H(A,B)HA​(B)​=H(A)+HA​(B)=H(B)+HB​(A)=H(A,B)−H(A)​ AAAとBBBについての情報を知っている方が片方だけを知っているよりも大きいということです 相互情報量 AAAとBBBに関係がある時,AAAについての情報を得ればBBBについていくらか知ることができます この情報量を相互情報量I(A,B)I(A,B)I(A,B)とします I(A,B)=H(B)−HA(B) I(A,B)=H(B)-H_A(B) I(A,B)=H(B)−HA​(B) 相互情報量は次のような性質を持つ 1 I(A,B)=I(B,A) I(A,B)=I(B,A) I(A,B)=I(B,A) I(A,B)=H(B)−HA(B)=H(B)+H(A)−H(A,B)I(A,B)=H(B)-H_A(B)=H(B)+H(A)-H(A,B)I(A,B)=H(B)−HA​(B)=H(B)+H(A)−H(A,B)であり,H(B)−H(A,B)=HB(A)H(B)-H(A,B)=H_B(A)H(B)−H(A,B)=HB​(A)であるということからわかります AAAとBBBは同じように関係しているので片方を知ることで得られるもう片方への情報量は同じだということがわかる 2 I(A,B)≤H(A),H(B) I(A,B)\\leq H(A),H(B) I(A,B)≤H(A),H(B) 定義より明らかです 3 I(A,B)≤0 I(A,B)\\leq 0 I(A,B)≤0 AAAを知りBBBについての情報が増えることはあっても減ることはないので理解できると思います 4 I(A,B∣C)≥0 I(A,B|C) \\geq 0 I(A,B∣C)≥0 ここでは3つの事象系に関する情報量を扱っています I(A,B∣C)=∑i,j,kp(Ck)pCk(Ai,Bj)log⁡pCk(Ai,Bj)pCk(Ai)pCk(Bj)I(A,B|C)=\\sum_{i,j,k}p(C_k)p_{C_k}(A_i,B_j)\\log\\frac{p_{C_k}(A_i,B_j)}{p_{C_k}(A_i)p_{C_k}(B_j)}I(A,B∣C)=∑i,j,k​p(Ck​)pCk​​(Ai​,Bj​)logpCk​​(Ai​)pCk​​(Bj​)pCk​​(Ai​,Bj​)​と定義します これはC=CkC=C_kC=Ck​だとわかったときのAAAとBBBとの相互情報量の平均値で,CCCがなんであるかわかったときにAAAを知って得られるBBBに関する情報量です.これが000より大きくなるのは上と同じ理由からです 5 I(AB,C)=I(A,C)+I(B,C∣A) I(AB,C)=I(A,C)+I(B,C|A) I(AB,C)=I(A,C)+I(B,C∣A) ABABABを知ってからCCCについて得る情報量は,AAAを知ってからCCCについて得る情報量と,AAAを知った後でBBBを知ってからCCCについて得る情報量を足した情報量と等しいということです 6 I(AB,C)≥I(A,C) I(AB,C) \\geq I(A,C) I(AB,C)≥I(A,C) これはAAAだけでなくBBBについても知って得る情報量が減るということはないです 7 I(A,B,C)=I(B,C,A)=... I(A,B,C)=I(B,C,A)=... I(A,B,C)=I(B,C,A)=... 相互情報量は関係のある事象系の数が増えても対称性が成り立つということです これは相互情報量の定義から当然なはずです "},"info-theo/info-theo02.html":{"url":"info-theo/info-theo02.html","title":"02 Information Resource","keywords":"","body":"02 Information Resource 情報源のモデル 今まで確率的に生起する事象を扱ってきましたが,実際の事象は一回で終わらず事象が系列となり,次々と起こります しかも,これから出てくる事象は今までに発生した事象に依存して確率的に定まることが多いです このようにこれから出てくる情報が過去の情報に依存して確率的に定まるものを情報源と言います 情報源が時刻1ごとに1つの事象を発生するとします 情報源はkkk個の事象A1,A2,…,AkA_1, A_2,\\ldots,A_kA1​,A2​,…,Ak​を手持ちとして,このうちの一つを出すと考えます 天気の場合,事象それぞれが天候の名称です.A1=晴れA_1=\\text{晴れ}A1​=晴れ, A2=曇りA_2=\\text{曇り}A2​=曇りとすると事象A1A_1A1​が連続して何回も起こるというのは確率的に難しいでしょう(もちろんここでは単純化して話しています) 英語の例(情報源のモデル) 英語の場合,情報源の手持ちの事象にはアルファベット26文字と句読点があり,これらが確率的に出ていると考えれば良いです 一般化 以降ではkkk個の事象\\ldotsと言う代わりに現れる文字の全体{A1,A2,…,Ak}\\{A_1,A_2,\\ldots,A_k\\}{A1​,A2​,…,Ak​}のことをアルファベットと呼びます 天気の場合は｢晴れ｣や｢曇り｣など天候の名称がアルファベットです 時刻ttt(t=1,2,3,…t=1,2,3,\\ldotst=1,2,3,…)に発生する文字をxtx_txt​で表すとします.これがAiA_iAi​ならば, xt=Ai x_t=A_i xt​=Ai​ です それ以前に出てきたsss個(s=1,2,3,…s=1,2,3,\\ldotss=1,2,3,…)の文字の系列が xt−s…xt−2xt−1 x_{t-s}\\ldots x_{t-2}x_{t-1} xt−s​…xt−2​xt−1​ であるとき,時刻tttに文字xtx_txt​の出る確率は p(xt∣xt−s…xt−1) p(x_t \\mid x_{t-s}\\ldots x_{t-1}) p(xt​∣xt−s​…xt−1​) と書くことにします また,時刻t−s∼tt-s\\sim tt−s∼tに文字系列 xt−s…xt−1xt x_{t-s}\\ldots x_{t-1}x_t xt−s​…xt−1​xt​ が出る確率は p(xt−r…xt) p(x_{t-r}\\ldots x_t) p(xt−r​…xt​) と書くことにします このとき p(xt−s…xt−1xt=p(xt∣xt−s…xt−1)p(xt−s…xt−1) p(x_{t-s}\\ldots x{t-1}x{t}=p(x_t \\mid x_{t-s}\\ldots x_{t-1})p(x_{t-s}\\ldots x_{t-1}) p(xt−s​…xt−1xt=p(xt​∣xt−s​…xt−1​)p(xt−s​…xt−1​) です これで文字系列を発生し続ける情報源の確率的な性質がわかります 定常的な情報源 ここでは情報源の性質が変わらないことを仮定しています 例えば,情報源が英語から日本語に変わってしまったらこれまでの文字系列が持つ意味はほとんど意味がなくなってしまいます このような情報源の性質が時間が経っても変わらないことは,条件付き確率p(xt∣xt−sxt−1)p(x_t \\mid x_{t-s}x_{t-1})p(xt​∣xt−s​xt−1​)はtttがなんであっても同じです つまりr=1,2,…r=1,2,\\ldotsr=1,2,…に対して p(xt+r)=p(xt)p(x_{t+r})=p(x_t)p(xt+r​)=p(xt​) p(xt+r∣xt+r−s…xt+r+1)=p(xt∣xt−s…xt−1)p(x_{t+r} \\mid x_{t+r-s}\\ldots x_{t+r+1}) = p(x_t \\mid x_{t-s}\\ldots x_{t-1})p(xt+r​∣xt+r−s​…xt+r+1​)=p(xt​∣xt−s​…xt−1​) を意味します このような情報源のことを定常的な情報源と言います 情報源のエントロピー 時刻tttに文字xtx_txt​が出る確率はp(xt)p(x_t)p(xt​)だから,このエントロピーは H(Xt)=−∑p(xt)log⁡p(xt) H(X_t)= -\\sum p(x_t)\\log p(x_t) H(Xt​)=−∑p(xt​)logp(xt​) で与えられます 大文字XtX_tXt​は時刻tttのxtx_txt​を表す確率変数です 情報源は定常的だからこれは H(X)=−∑xp(x)log⁡p(x) H(X) = - \\sum_x p(x)\\log p(x) H(X)=−x∑​p(x)logp(x) と書けます.また H(X)=∑ip(x=Ai)log⁡p(x=Ai) H(X) = \\sum_i p(x=A_i)\\log p(x=A_i) H(X)=i∑​p(x=Ai​)logp(x=Ai​) と書けます これは情報源1文字あたりのエントロピーではありません.なぜなら情報源から出てくる文字の系列を考える場合,xtx_txt​はxt−1,xt−2,…x_{t-1}, x_{t-2},\\ldotsxt−1​,xt−2​,…の文字と相互に影響していてその影響を無視できないからです そこで,情報源から出る相続いた2文字x1x2x_1 x_2x1​x2​を見ます 2文字のエントロピーは H(X1X2)=−∑x1x2p(x1x2)log⁡p(x1x2) H(X_1 X_2)= - \\sum_{x_1 x_2}p(x_1 x_2)\\log p(x_1 x_2) H(X1​X2​)=−x1​x2​∑​p(x1​x2​)logp(x1​x2​) と書けます 2文字の間に相関がある場合,このエントロピーは文字を1つずつ独立に見たときのエントロピー2H(X)2H(X)2H(X)よりは小さいです.つまり H(X1X2)≤2H(X) H(X_{1}X_{2}) \\leq 2H(X) H(X1​X2​)≤2H(X) 同様に,nnn個の相続いた文字の系列を見たときのエントロピーは H(Xn)=−∑xnp(xn)log⁡p(xn) H(X^n) = - \\sum_{x^n}p(x^n)\\log p(x^n) H(Xn)=−xn∑​p(xn)logp(xn) です.ただし,ここでは Xn=X1X2…Xnxn=x1x2…xn \\begin{aligned} X^n &= X_1 X_2\\ldots X_n\\\\ x^n &= x_1 x_2\\ldots x_n \\end{aligned} Xnxn​=X1​X2​…Xn​=x1​x2​…xn​​ です ここではnnn個でH(Xn)H(X^n)H(Xn)のエントロピーを1文字ずつに割り振ったとき Hn=H(xn)n H_{n}=\\frac{H(x^{n})}{n} Hn​=nH(xn)​ が1つ当たりのエントロピーとなります nnn個の文字xn=x1x2…xnx^n=x_1 x_2\\ldots x_nxn=x1​x2​…xn​の間に相関がなければ,これはH(X)H(X)H(X)に等しいが,相関があればHnH_nHn​はnnnと共に減少していきます これは｢日本大学経済学｣の次の文字が｢部｣だと予想がつくことからもわかるでしょう ここで情報源の1文字当たりのエントロピーHHHを H=lim⁡n→∞Hn H=\\lim_{n \\to \\infty} H_n H=n→∞lim​Hn​ と定義します.これは情報源から出る文字系列のずっと長い間見続けたいときの1文字当たりのエントロピーです 次のようにも考えられます.今までに出たnnn個の文字を覚えているとして,次の文字がなんであるかと考えるときのエントロピー,条件付きエントロピーは H(X∣Xn)=−∑p(xn)p(x∣xn)log⁡p(x∣xn) H(X \\mid X^n) = - \\sum p(x^n) p(x \\mid x^n) \\log p(x \\mid x^n) H(X∣Xn)=−∑p(xn)p(x∣xn)logp(x∣xn) となります.このとき情報源のエントロピーを lim⁡n→∞H(X∣Xn)=H(X∣X∞) \\lim_{n \\to \\infty}H(X \\mid X^n) = H(X \\mid X^{\\infty}) n→∞lim​H(X∣Xn)=H(X∣X∞) で定義します.これもnnnと共に単調減少します これは先に定義したHHHに収束します この証明は H(Xn)=H(X)+H(X∣X)+H(X∣X2)+…+H(X∣Xn−1) H(X^n)=H(X)+H(X \\mid X)+H(X \\mid X^2)+\\ldots+H(X \\mid X^{n-1}) H(Xn)=H(X)+H(X∣X)+H(X∣X2)+…+H(X∣Xn−1) からすることが可能です 英語の例(情報源のエントロピー) 英語のアルファベットの26個が全て等確率で独立に発生すると仮定すると,このエントロピーは H0=log⁡226=4.7ビット H_0 = \\log_2 26 = 4.7\\text{ビット} H0​=log2​26=4.7ビット です 実際の出現頻度を調べた結果を元にすると H1=4.15ビット H_1 = 4.15\\text{ビット} H1​=4.15ビット となります.しかしこれも前後の文字の影響が入っていないのでそれも考慮すると,英語の2文字の組み合わせx2=x1x2x^2=x_1 x_2x2=x1​x2​の全ての頻度を調べ,H2H_2H2​を求めると H2=3.57ビット H_2 = 3.57\\text{ビット} H2​=3.57ビット となります.同様に8文字の文字系列x8=x1x2…x8x_8 = x_1 x_2 \\ldots x_8x8​=x1​x2​…x8​の全ての出現頻度を調べると H8=18H(X8)=−18∑p(x8)log⁡p(x8) H_8 = \\frac{1}{8} H(X^8) = - \\frac{1}{8}\\sum p(x^8)\\log p(x^8) H8​=81​H(X8)=−81​∑p(x8)logp(x8) と計算できるようです.これによると H8=2.35ビット H_8 = 2.35 \\text{ビット} H8​=2.35ビット となるようです しかし,現実では文章の始めの方では話の予想はつかないが,最後の方では予想が付くことが多いです(推理小説など) そのため,大方の予想では H=1.3ビット H=1.3\\text{ビット} H=1.3ビット 程度ということになっているそうです "},"info-theo/info-theo03.html":{"url":"info-theo/info-theo03.html","title":"03 Marcov chain","keywords":"","body":"03 Marcov chain マルコフ的情報源 これまでに扱ったものは実際問題として求めるのは非常に困難です 加えて,現実の情報源を忠実に表しているのではなく近似をしているに過ぎません どうせ近似ならばもっと扱いやすいものにした方がよいということで登場するのがマルコフ的情報源です マルコフ的情報源とはいくつかの内部状態を持った情報源のことです この内部状態をS1,S2,…,SrS_1, S_2,\\ldots,S_rS1​,S2​,…,Sr​で表します 今,SαS_{\\alpha}Sα​にあったとします.すると次の時刻には情報源は確率pβαp_{\\beta \\alpha}pβα​で状態SβS_{\\beta}Sβ​に遷移し文字AiA_iAi​を発生します 状態SαS_{\\alpha}Sα​にあるときに文字AiA_iAi​を発生する確率をqiαq_{i \\alpha}qiα​とします 今度は状態SβS_{\\beta}Sβ​にいます.このとき確率確率pγβp_{\\gamma \\beta}pγβ​で状態SγS_{\\gamma}Sγ​に遷移し,文字AjA_jAj​を発生します 状態SβS_{\\beta}Sβ​にあるときに文字AjA_jAj​の出る確率はqjβq_{j \\beta}qjβ​です このように逐次確率的に状態を移行しながら文字を発生し発生する文字の確率分布はその時の状態に依存しているような情報源をマルコフ的情報源と言います マルコフ的情報源は状態遷移図で説明するとわかりやすいです 以下の図は状態がS1,S2,S3S1,S2,S3S1,S2,S3の3つであるようなマルコフ的情報源を表す遷移図です 状態は丸で囲われる 状態の遷移の確率は矢印の上に書かれている という特徴が有ります 図1では 状態に関して S1からS1になる確率p11p_{11}p11​は0.40.40.4 S1からS2になる確率p12p_{12}p12​は0.30.30.3 S1からS3になる確率p13p_{13}p13​は0.30.30.3 文字に関して S1から遷移する時に文字A1A_1A1​が出る確率は0.70.70.7 などがわかります マルコフ的情報源ではこれから発生する文字の確率分布は現在の情報源の状態だけで定まるので議論がしやすいです これ以前に紹介した情報源も過去に出たsss個の文字の組Ai1Ai2…AisA_{i1}A_{i2}\\ldots A_{is}Ai1​Ai2​…Ais​をひとまとめにしたものを情報源の内部状態と考えることができます つまりに過去にAi1Ai2…AisA_{i1}A_{i2}\\ldots A_{is}Ai1​Ai2​…Ais​の文字系列が出たということは情報源が今状態Sα=Ai1…AisS_{\\alpha}=A_{i1}\\ldots A_{is}Sα​=Ai1​…Ais​にあると見ることができます.次に文字AiA_iAi​が出る確率qiαq_{i\\alpha}qiα​は qiα=p(Ai∣Ai1…Ais) q_{i\\alpha} = p(A_i \\mid A_{i1}\\ldots A_{is}) qiα​=p(Ai​∣Ai1​…Ais​) と書けます 文字AiA_{i}Ai​が出た後に文字系列にAiA_iAi​が新たに付け加えられ,その代わりに一番古い文字Ai1A_{i1}Ai1​が削除されるので情報源の状態は sα=Ai1Ai2…Ais→sβ=Ai2…AisAi s_{\\alpha} = A_{i1}A_{i2}\\ldots A_{is} \\to s_{\\beta} = A_{i2}\\ldots A_{is}A_{i} sα​=Ai1​Ai2​…Ais​→sβ​=Ai2​…Ais​Ai​ に変わります.この状態遷移の確率pβαp_{\\beta \\alpha}pβα​は文字AiA_iAi​の出る確率qiαq_{i \\alpha}qiα​に等しいです 文字の出現確率が無限の過去を元によるわけではないので,ここではsssの値を十分に大きく取ればマルコフ的情報源に近似することができると考えられています しかし,sssが100や1000のように大きな数だと状態が多すぎて確率の計算が大変になってしまいます.あくまで｢理論上｣の話であり,理論上の考察は楽になります マルコフ連鎖 マルコフ的情報源では過去の影響は全て状態の中に取り入れられるので,状態の推移を調べることが重要です このように現在の状態に依存して確率的に状態が遷移していく過程のことをマルコフ連鎖と言います 図2のような状態遷移図を持つマルコフ連鎖を例にその性質を考えてみます このマルコフ連鎖は7つの状態から成ります ここでBという枠に囲まれた状態に落ち込むと外に出ることはできません.このような性質を持つ状態の部分集合をマルコフ連鎖の閉部分集合と言います B以外にCも閉部分集合です Bに落ち込んでしまうと外の状態に行くことはできませんが,B内部の状態は相互に行くことができます.このようにお互いにどの状態からどの状態へも行くことのできる閉部分集合のことを,強連結成分と言います Aに含まれる状態を考えます.始めの状態がAの中にあったとしても,時間が立てばいつかは必ずBかCの中に移ってしまいます.このように状態がいつまでもA内にとどまっている確率が0であるような状態のことを消散部分と言います マルコフ連鎖の状態は消散部分といくつかの強連結成分に分けられます.図2ではAが消散部分,BとCが閉強連結成分です マルコフ連鎖で表される情報源を問題として,この情報源から出る情報の統計的性質を調べようとしているので,時間が経てばなくなってしまう消散部分は無視をしても良いでしょう これからは閉強連結成分のみを考えることにします マルコフ連鎖では全体が1つの閉強連結成分になっているものを分解不可能と言います.一方,いくつかの部分に分解できるものを分解可能と言います これからは分解不可能なもののみを扱います 周期的なマルコフ連鎖 分解不可能なマルコフ連鎖にも次の図3のような変わり種があります このマルコフ連鎖は以下のような状態をもちます ある状態から出発し,元の状態に帰るループに含まれる枝の数が2の倍数 時刻1にS1S_1S1​にいるとき 偶数時刻はS2S_2S2​orS4S_4S4​ 奇数時刻はS1S_1S1​orS3S_3S3​ これを周期的な連鎖と良います ループの枝の数の最大公約数mmmを連鎖の周期 公約数がないとき,非周期的なマルコフ連鎖と言います 各状態にどのくらいの頻度で現れるかを調べるために状態感の遷移確率pβαp_{\\beta \\alpha}pβα​を並べて,遷移確率行列PPPを定義します P=[p11……pγα⋮⋱pβα⋮⋮⋱⋮pγ1……pγγ] P = \\left[ \\begin{array}{cccc} p_{11} & \\ldots & \\ldots & p_{\\gamma \\alpha} \\\\ \\vdots & \\ddots & p_{\\beta \\alpha} & \\vdots \\\\ \\vdots & & \\ddots & \\vdots \\\\ p_{\\gamma 1} & \\ldots & \\ldots & p_{\\gamma \\gamma} \\end{array} \\right] P=⎣⎢⎢⎡​p11​⋮⋮pγ1​​…⋱…​…pβα​⋱…​pγα​⋮⋮pγγ​​⎦⎥⎥⎤​ と書けます.ただし,β\\betaβ行α\\alphaα列要素はα\\alphaαからβ\\betaβへ移る確率を表します そして (p2)βα=∑γpβγpγα (p^2)_{\\beta \\alpha} = \\sum_{\\gamma}p_{\\beta \\gamma}p_{\\gamma \\alpha} (p2)βα​=γ∑​pβγ​pγα​ となります(γ\\gammaγは任意の状態) 分解可能で非周期的なマルコフ連鎖では次の補題が成立します 補題 p∞=lim⁡n→∞pn p^{\\infty} = \\lim_{n \\to \\infty}p^n p∞=n→∞lim​pn が存在し,p∞p^{\\infty}p∞は同一の列ベクトルu\\boldsymbol{u}uより成ります.つまり p∞=[u,u,…,u],u=(u1u2⋮ur) p^{\\infty} = [\\boldsymbol{u}, \\boldsymbol{u},\\ldots, \\boldsymbol{u}], \\boldsymbol{u} = \\left( \\begin{array}{c} u_1\\\\ u_2\\\\ \\vdots\\\\ u_r \\end{array} \\right) p∞=[u,u,…,u],u=⎝⎜⎜⎛​u1​u2​⋮ur​​⎠⎟⎟⎞​ このu\\boldsymbol{u}uは Pu=u∑ui=1,ui≥0 \\begin{aligned} P\\boldsymbol{u} = \\boldsymbol{u}\\\\ \\sum u_i = 1, u_i \\geq 0 \\end{aligned} Pu=u∑ui​=1,ui​≥0​ を満たすベクトルで,これより一意に定まります 十分に時間が経てば,どこから出発してもSαS_{\\alpha}Sα​である確率はuαu_{\\alpha}uα​であり,この確率を定常確率と言います 分解不可能でも周期的なマルコフ連鎖ではp∞p^{\\infty}p∞は存在しません これはpnp^npnはnnnが大きくなると,周期mmmの振動を起こし収束しないからです.しかし,u\\boldsymbol{u}uは存在し,同じ式から求めることができます マルコフ連鎖のエントロピー 次にマルコフ的情報源のエントロピーを求めます 今,状態SαS_{\\alpha}Sα​にいるとして,次に出る文字のエントロピーは Hα(X)=−∑iqiαlog⁡iα H_{\\alpha}(X) = - \\sum_{i} q_{i \\alpha} \\log i \\alpha Hα​(X)=−i∑​qiα​logiα 長い時間かかるほどSαS_{\\alpha}Sα​にいる確率は定常状態uαu_{\\alpha}uα​になりエントロピーがHα(X)H_{\\alpha}(X)Hα​(X)となる確率はuαu_{\\alpha}uα​.よって全体の1文字当たりのエントロピーは H=∑αuαHα(X) H = \\sum_{\\alpha}u_{\\alpha}H_{\\alpha}(X) H=α∑​uα​Hα​(X) は,状態がなんであるかわかるという条件での1文字当たりのエントロピー Hs(X)=−∑αuαqiαlog⁡iα H_s(X) = - \\sum_{\\alpha}u_{\\alpha}q_{i \\alpha}\\log i \\alpha Hs​(X)=−α∑​uα​qiα​logiα に等しいです 情報源の冗長度 アルファベットをkkk個持つ情報源のエントロピーは1文字当たり最大 H0=log⁡k H_0 = \\log k H0​=logk となります.現実には1文字当たりHHHのエントロピーしか出していないとすると, r=H0−HH0 r = \\frac{H_0 - H}{H_0} r=H0​H0​−H​ だけの情報を損しています.このrrrを冗長度と言います これは無駄なく情報源を用いれば字数がrrr%節約できるということです 情報源の大数の法則 文字系列x1x2…xNx_1 x_2\\ldots x_Nx1​x2​…xN​の出現確率をp(x1x2…xN)p(x_1 x_2\\ldots x_N)p(x1​x2​…xN​)と書きます そして,xix_ixi​が出現するところで,今までにx1x2…xi−1x_1 x_2 \\ldots x_{i-1}x1​x2​…xi−1​という文字系列が発生したとします.このときxix_ixi​の発生する確率をp(xi∣…xi−2xi−1)p(x_i \\mid \\ldots x_{i-2}x_{i-1})p(xi​∣…xi−2​xi−1​)と書きます このxix_ixi​を観測することで得られる情報量は Ii=log⁡p(xi∣…xi−2xi−1) I_i = \\log p(x_i \\mid \\ldots x_{i-2}x_{i-1}) Ii​=logp(xi​∣…xi−2​xi−1​) であり,IiI_iIi​の期待値をIiˉ(…xi−2xi−1)\\bar{I_i}(\\ldots x_{i-2}x_{i-1})Ii​ˉ​(…xi−2​xi−1​)として Iiˉ(…xi−2xi−1)=∑xip(xi∣…xi−2xi−1)Ii=−∑xip(xi∣…xi−2xi−1)log⁡p(xi∣…xi−2xi−1) \\begin{aligned} \\bar{I_i}(\\ldots x_{i-2}x_{i-1}) &= \\sum_{x_i}p(x_i \\mid \\ldots x_{i-2}x_{i-1})I_i \\\\ &= - \\sum_{x_i}p(x_i \\mid \\ldots x_{i-2}x_{i-1})\\log p(x_i \\mid \\ldots x_{i-2}x_{i-1}) \\end{aligned} Ii​ˉ​(…xi−2​xi−1​)​=xi​∑​p(xi​∣…xi−2​xi−1​)Ii​=−xi​∑​p(xi​∣…xi−2​xi−1​)logp(xi​∣…xi−2​xi−1​)​ これを更に文字系列…xi−2xi−1\\ldots x_{i-2}x_{i-1}…xi−2​xi−1​について平均したものが情報源の1文字当たりのエントロピー H=∑…xi−1∑xip(…xi−1)p(xi∣…xi−1)log⁡p(xi∣…xi−1) H = \\sum_{\\ldots x_{i-1}}\\sum_{x_i}p(\\ldots x_{i-1})p(x_i \\mid \\ldots x_{i-1})\\log p (x_i \\mid \\ldots x_{i-1}) H=…xi−1​∑​xi​∑​p(…xi−1​)p(xi​∣…xi−1​)logp(xi​∣…xi−1​) となります.過去の文字系列が全て分かっているとき,これから更に長さNNNのも時系列x1x2…xNx_1 x_2\\ldots x_Nx1​x2​…xN​を見て得られる情報量IIIは I=−log⁡p(x1x2…xN∣x∞) I = - \\log p(x_1 x_2\\ldots x_N \\mid x^{\\infty}) I=−logp(x1​x2​…xN​∣x∞) ただし p(x1x2…xN∣x∞)=p(x1∣x∞)…p(xN∣x∞) p(x_1 x_2\\ldots x_N \\mid x^{\\infty}) = p(x_1 \\mid x^{\\infty}) \\ldots p(x_N \\mid x^{\\infty}) p(x1​x2​…xN​∣x∞)=p(x1​∣x∞)…p(xN​∣x∞) このことから I=−∑i=1NIi I = - \\sum_{i=1}^{N}I_i I=−i=1∑N​Ii​ と書けます このとき,IiI_iIi​は全て平均値がHHHの確率変数です.NNN個集めるとおおよそNHNHNHに等しくなるので I≃NH I \\simeq NH I≃NH これは確率論の大数の法則に相当します 実際にNNN文字の系列1つを見て得られる情報量IIIはちょうどNHNHNHにはなリマ千.むしろNNNを大きくするとNHNHNHからは外れてしまいます.しかし1/N1/N1/NはHHHに近付きます 1N→H \\frac{1}{N} \\to H N1​→H また,どんな小さい正の数ε\\varepsilonεに対してもNNNを十分大きく取れば実際に観測される文字系列の情報量はほとんど I≈(H±ε)N I \\approx (H \\pm \\varepsilon)N I≈(H±ε)N に収まります.そのため今後は近似的にI=NHI=NHI=NHとします. ここで,IIIは文字系列の出現確率をpppとすると I=−log⁡p I = - \\log p I=−logp です.そのため log⁡p≈(H±ε)N \\log p \\approx (H \\pm \\varepsilon)N logp≈(H±ε)N が導けます.長さNNNのも時系列は全部でkNk^NkN個ありますが,この文字系列の全体LLLを2つの部分L1L_1L1​とL2L_2L2​に分けてみます 長さNNNのも時系列のうち,出現確率pppが (H−ε)N−log⁡p(H+ε)N (H - \\varepsilon)N (H−ε)N−logp(H+ε)N すなわち, ∣−log⁡pN−H∣ε \\left| \\frac{-\\log p}{N} - H \\right| ∣∣∣∣​N−logp​−H∣∣∣∣​ε を満たすものを全体のL1L_1L1​とし,出現確率pppが ∣−log⁡pN−H∣≥ε \\left| \\frac{-\\log p}{N} - H \\right| \\geq \\varepsilon ∣∣∣∣​N−logp​−H∣∣∣∣​≥ε となるものを全体のL2L_2L2​とします つまり,L1L_1L1​の方に入文字系列の出現確率は p≃2−NH p \\simeq 2^{-NH} p≃2−NH であり,L2L_2L2​の方に入る確率は2−NH2^{-NH}2−NHよりは小さいです たとえε\\varepsilonεを非常に小さく選んでも,NNNが十分に大きければL1L_1L1​に属する文字系列ばかりが出てきて,L2L_2L2​に属するものは例外程度とないります.これを定理化すると次のようになります 定理 任意のε>0,δ>0\\varepsilon > 0, \\delta > 0ε>0,δ>0に対して,あるN0N_0N0​が存在し,N≥N0N \\geq N_0N≥N0​ならL1L_1L1​に属する文字系列の発生する確率を1−δ1-\\delta1−δより大きくすることができる すなわち文字系列の出現確率をpppとして Prob{∣−log⁡pN−H∣≥ε}δ Prob \\left\\{ \\left| \\frac{-\\log p}{N} - H \\right| \\geq \\varepsilon \\right\\} Prob{∣∣∣∣​N−logp​−H∣∣∣∣​≥ε}δ この定理はNNNを十分大きく取ればL2L_2L2​の部分は無視してしまってL1L_1L1​の部分のみを考えても差し支えがないということを述べています.L1L_1L1​に属する系列の出現確率は p=2−NH p = 2^{-NH} p=2−NH にほぼ等しいので,L1L_1L1​にはほぼ 1÷2−NH=2NH 1 \\div 2^{-NH} = 2^{NH} 1÷2−NH=2NH の文字系列が含まれいてることがわかります.これはNNNが十分大きい時にL1L_1L1​に含まれている文字系列の総数は2(H+ε)N2^{(H+\\varepsilon)N}2(H+ε)N以下であるということです アルファベットの文字数をkkkとして,LLLに属する文字列の総数は kN=2H0NH0Nlog⁡22=Nlog⁡2KH0=log⁡k \\begin{aligned} k^N &= 2^{H_0 N}\\\\ H_0 N \\log_2 2 &= N \\log_2 K\\\\ H_0 &= \\log k \\end{aligned} kNH0​Nlog2​2H0​​=2H0​N=Nlog2​K=logk​ であり,HH0HHH0​とするとNNNが大きければ 2H0N≫2HN 2^{H_0 N} \\gg 2^{HN} 2H0​N≫2HN だからL2L_2L2​に含まれる文字系列の数はL1L_1L1​に含まれるものよりもずっと大きいです.そのため,その比ほとんど000となりますが,実際に出現するのは数的には圧倒的に少ないL1L_1L1​のものばかりだという性質を示しています エルゴード性 集団平均と時間平均が等しくなる時,エルゴード的と良い,このような時に上での議論が成り立ちます この性質,エルゴード性はあるサイコロDDDをある人P0P_0P0​が100010001000回振った時と,サイコロDDDを1000人P1,…,P1000P_1,\\ldots,P_{1000}P1​,…,P1000​が同時に111回振った時に出る値の平均が変わらないということを表します(サイコロが分裂しない限りありえないですが,あくまで例え話として) 分解可能で非周期的なマルコフ連鎖はエルゴード性を持ち,そのため定常状態を持つのです "},"info-theo/info-theo04.html":{"url":"info-theo/info-theo04.html","title":"04 Discrete communication path","keywords":"","body":"04 Discrete communication path "},"info-geo/info-geo00.html":{"url":"info-geo/info-geo00.html","title":"Information geometry","keywords":"","body":"Information geometry 情報理論･統計学の問題を幾何学的に解釈する学問です Basic math "},"info-geo/info-geo01.html":{"url":"info-geo/info-geo01.html","title":"01 Basic math","keywords":"","body":"01 Basic math これ以降に用いる基本事項についてこのページではまとめます 1. 逆写像定理 領域D⊂RlD \\subset \\mathbb{R}^lD⊂Rlで定義された写像 f:D⟶Rn:x=t(x1,...,xl)⟼t(f1(x),...,fn(x)) \\bm{f}: D \\longrightarrow \\mathbb{R}^{n} : \\bm{x} = {}^t\\!(x_1,...,x_l) \\longmapsto {}^t\\!(f_1(\\bm{x}),...,f_n(\\bm{x})) f:D⟶Rn:x=t(x1​,...,xl​)⟼t(f1​(x),...,fn​(x)) がx=a\\bm{x}=\\bm{a}x=aで微分可能(全微分可能)とはあるn×ln \\times ln×l行列AAAが存在して f(a+h)=f(a)+Ah+o(∥h∥) \\bm{f}(\\bm{a}+\\bm{h}) = \\bm{f}(\\bm{a})+ A \\bm{h}+o(\\|\\bm{h}\\|) f(a+h)=f(a)+Ah+o(∥h∥) と書くことができます.このときAAAはf\\bm{f}fのJacobi行列 ∂f∂x=[∂f1∂x1…∂f1∂xl⋮⋮∂fn∂x1…∂fn∂xl] \\frac{\\partial \\bm{f}}{\\partial \\bm{x}} = \\left[ \\begin{array}{ccc} \\frac{\\partial f_1}{\\partial x_1} & \\ldots & \\frac{\\partial f_1}{\\partial x_l}\\\\ \\vdots & & \\vdots\\\\ \\frac{\\partial f_n}{\\partial x_1} & \\ldots & \\frac{\\partial f_n}{\\partial x_l} \\end{array} \\right] ∂x∂f​=⎣⎢⎡​∂x1​∂f1​​⋮∂x1​∂fn​​​……​∂xl​∂f1​​⋮∂xl​∂fn​​​⎦⎥⎤​ のx=a\\bm{x} = \\bm{a}x=aでの値に一致します 陰関数定理 以下は陰関数定理というある条件を満たす時に陰関数が存在するということを示す定理です 写像F\\bm{F}F: D(⊂Rk×Rn)⟶RnD(\\subset \\mathbb{R}^k \\times \\mathbb{R}^n)\\longrightarrow \\mathbb{R}^nD(⊂Rk×Rn)⟶RnがC1C^1C1級で F(a,b)=0 \\bm{F}(\\bm{a}, \\bm{b}) = \\bm{0} F(a,b)=0 を満たし,かつその点におけるJacobi行列 ∂F∂y(a,y) \\frac{\\partial \\bm{F}}{\\partial \\bm{y}}(\\bm{a}, \\bm{y}) ∂y∂F​(a,y) が正則であるとします.このときx=a\\bm{x} = \\bm{a}x=aのある近傍U(⊂Rk)U(\\subset \\mathbb{R}^k)U(⊂Rk)とあるC1C^1C1級関数f:U⟶Rnf:U\\longrightarrow R^nf:U⟶Rnが存在して全てのx∈U\\bm{x} \\in Ux∈Uで F(x,f(x))=0 \\bm{F}(\\bm{x}, \\bm{f}(\\bm{x})) = \\bm{0} F(x,f(x))=0 が成り立ちます 逆写像定理 以下は逆写像定理という与えられた写像の逆写像の存在条件を明らかにする定理です 点a=(a1,...,an)\\bm{a}=(a_1,...,a_n)a=(a1​,...,an​)を含む領域D⊂RnD\\subset \\mathbb{R}^nD⊂RnからRn\\mathbb{R}^nRnへのC1C^1C1級写像 f:D⟶Rn:x=(x1,...,xn)⟼y=(f1(x),...,fn(x)) \\bm{f}: D \\longrightarrow \\mathbb{R}^n : \\bm{x} = (x_1,...,x_n) \\longmapsto \\bm{y} = (f_1(\\bm{x}),...,f_n(\\bm{x})) f:D⟶Rn:x=(x1​,...,xn​)⟼y=(f1​(x),...,fn​(x)) が点f(a)∈Rnf(\\bm{a})\\in \\mathbb{R}^nf(a)∈Rnの近傍でC1C^1C1級の逆写像を持つための必要十分条件はf\\bm{f}fのJacobi行列∂f∂x\\frac{\\partial \\bm{f}}{\\partial \\bm{x}}∂x∂f​が点x=a\\bm{x}=\\bm{a}x=aで正則であること,すなわち det⁡(∂f∂x) \\det\\left( \\frac{\\partial \\bm{f}}{\\partial \\bm{x}} \\right) det(∂x∂f​) が点x=a\\bm{x} = \\bm{a}x=aで000ではないということです 2. 双対空間 3. テンソル "},"os/os00.html":{"url":"os/os00.html","title":"Operating system","keywords":"","body":"Operating system Operating System(OS)とはコンピュータの基盤となるようなシステムです Introduction to OS Thread & Process C & Assembly Parallelism & Synchronization "},"os/os01.html":{"url":"os/os01.html","title":"01 Introduction to OS","keywords":"","body":"01 Introduction to Operating System 以下にはコードが登場します Cをコンパイルした結果のアセンブリが見たいときはhttps://gcc.godbolt.org/を使ってください OSとは OSはコンピュータを動かす際に最も基本的なソフトウェアです 情報がどの記憶装置にあるのかという違いを考慮せずにプログラムを書くことを可能にします 同時に複数のプログラムが,プログラマーがいちいち書かずに動かせるようになります ブラウザ,メールクライアント,ゲームなどを並行して動かせます バグによってメモリが破壊されないようになっています 実例 Windows, Linux, Solaris, BSD, Mach,... Mac OSはBSDがベース AndroidはLinuxがベース Windows以外のOSはUnixというOSから派生 なぜ学ぶのか OSは根幹的なシステムなのでこれを学ぶことで多くの応用へと繋がります 実用プログラムの基本 OSの根本的な部分の勉強を勉強するとネットワークをプログラムから利用可能にするソフトウェアが書けるようになる ライブラリ,プログラミング言語などで使われる機能の実装方法を理解できます セキュリティへのより深い理解 効率的なプログラムとそうでないプログラムを理解することができます OSは高速にプログラムの間を切り替えて実行おり,その中の動きを理解しなければプログラムに対する選択の良し悪しがわかりません OSの細かい機能はどうでも良いという人 OSののAPIを組み合わせる際に,OSを持つ機能を理解していれば必要に応じてマニュアルを見るという正しい手法をすることができます マニュアルを読む際にOSの背景知識が必要になることが多く,マニュアルを読んで理解することも可能です OSの役割 OSには以下のような役割が有ります 計算機資源を抽象化･仮想化します プロセス間の隔離,計算機資源の管理 抽象化･仮想化(便利な側面) ハードウェアとソフトウェアの間に入るような機能を提供しています(仮想化) ナマのハードウェアではやりにくい出入力などを扱ったり,OSを簡単に扱えるようなAPIを提供しています また,実際は複数のプログラムで計算資源を共有しているのですが,それを意識しなくても使うことができます(抽象化) CPUやメモリでプログラムの資源の割当をうまくしています プロセス間の隔離,計算機資源の管理(安全な側面) プロセス間の隔離とは他の人が自分のプログラムを閲覧したり,破壊できないようにする機能です 計算資源の管理とはあるユーザーがCPUやメモリを独占的に利用できないようにしています システムコール システムコールを呼び出すことで,実行が面倒な仕事(入出力,計算資源の割当)を実行できます また,システムコールを介してでないと資源を利用できないようにすることで安全性を保証しています 安全性をどう実現をするのか ファイルを読み書きするシステムコールはopen, read, writeです(便利な側面) これらを通してしかファイルにアクセスができないようにすれば安全です しかし,その保証は自明ではありません というのもディスクにアクセスするというプログラム,OSの動作の真似をすれば誰でもアクセスができそうです CPUの仕組み 命令･モード 命令には2種類あります 非特権命令 特権命令 CPUの動作状態にも2種類あります ユーザーモード(非特権) スーパバイザモード(特権) CPUによっては更に細分化する場合が有ります 特権命令(入出力命令,一部のメモリ領域へのアクセスなど)は特権モードでのみ実行可能とします これによって普段行っているプログラムは非特権命令なので,保護をすることができます トラップ命令 トラップ命令とは｢特権モードへ移行+特定番地へジャンプ｣という命令です ｢特権モードへ移行｣という命令と｢特定番地へジャンプ｣という命令を同時にすることで他の命令では実現できないようにできます トラップ命令はユーザモードで実現可能です 特定番地を決めるのは割り込みベクタと呼ばれるメモリ上の表です 割り込みベクタは特権命令によって設定されます 安全性のためのOSの仕組み アプリケーションが特権命令を実行しようと思った時は, システムコール(トラップ命令)→特権命令(非特権命令も使う) ここではアプリケーションとOSをシステムコールが結び,OSとCPUを特権命令が繋いでいます(非特権命令も) まとめ スレッドとプロセス(CPUの抽象化･管理) 仮想記憶,アドレス空間(メモリの抽象化･管理) ファイルシステム プロセス間通信(ソケット) 認証とセキュリティ "},"os/os02.html":{"url":"os/os02.html","title":"02 Thread & Process","keywords":"","body":"02 Thread and Process スレッド･プロセスの目的 CPUを仮想化することができます 物理的なCPUの数は固定,少数,有限 PCやスマホは大概8コアまで サーバーでも数十 しかし,それ以上のプログラムを立ち上げることが可能です ここのプログラムを書く人が明示的な譲り合いをする必要はありません スレッドとは スレッドとは制御の流れ OSが｢スレッドを作りたい｣と要求 OSがスレッドにCPUを割り当て実行 OSが交互にスレッド実行し,CPUが複数あれば各CPU上スレッドを作ります プロセスとは プロセスとはプログラムを起動したときにできるもの 言い換えるとプロセスとは｢論理アドレス空間の生成+mainスレッド(1つ以上のスレッド)の生成｣ 実際には Linux: ps, top, htop Windows: perfmon などで現在走っているプロセスを見ることが可能です 関連API スレッド･プロセスに関連するAPI 共通する主要概念 生成･終了 同期･実行の制御 代表的スレッドAPI Unix: POSIX Threads(pthread) Windows: Win32 thread 代表的プロセスAPI Unix: POSIX Windows: Win32 プロセス生成 Unix プロセスを作ること自体はforkで終わっています execvでは,現在のプロセスを新しいプロセスイメージで置き換え実行します pid = fork(); /* プロセス複製 */ if (pid == 0) { /* 子プロセス */ execv(file, cmdline); /* fileを実行 */ } else { /* 親プロセス */ } Windows fileをコマンドラインcmdで起動します CreateProcess(file, cmd, ...., &pid); プロセス終了 プロセスの終了はWindowsとUnixで以下のように行われます Win: ExitProcess(s) Unix: eixt(s) またはプログラムのmain関数が終了した時に自動的に終了します プロセス終了待ち プロセスpidが終了するのを待つAPIは以下の通りです Win: WaitForSingleObject(pid, timeout) Winは待つ時には同じAPIを使います Unix: wait(&status) waitpid(pid, &status,...) 高水準なAPI Unixの場合,最終的にはfork, exec, waitpidなどの組み合わせに過ぎません system(command_string) command_stringを実行するプロセスを作り終了を待ちます fork+exec+wait popen(command_string) command_stringを実行するプロセスを作り,そのプロセスと通信するチャネル(パイプ)を返します pipe作り+fork+exec スレッド生成 命令列が始まるアドレスを指定します そこから実行を開始するスレッドを生成します 命令列のアドレス=Cの関数ポインタ スレッド生成実例 f(x)を実行するスレッドを生成し,スレッド名をidに格納する関数は以下の通りです Unix: pthread_create(&id, ..., f, x) Wind: CreateThread(..., ..., f, x, ..., 'id) スレッド終了 sとは'status'のことです スレッド終了実例 Unix: pthread_exit(s) Win: ExitThread(s) また,生成時に指定された関数がfが終了すると自動的にスレッドは終了します スレッドの終了待ち スレッドidが終了するのを待ちます idはpthread_craeteなどによって得られたスレッド名です スレッド終了待ち実例 Unix: pthread_join(id, &return_status) Win: WaitforSingleObject(id, timeout) その他の言語 ほとんどの言語でスレッド,プロセス関係のAPIは提供されています (上ではC言語が例ですが, Java, C++, Pythonなどでも同様) 名前が異なることは多いですが,概念は似ています 多くの場合はC言語用のライブラリからfork, execなどを呼び出しているだけです(Unixの場合) プロセスとスレッド 両者は似ているように思えますが,実際に違います 現時点では スレッド...一筋の実行の流れ プロセス...プログラムを起動した時にできるもの 箱+1個以上のスレッド mainを実行するスレッドはプロセスと共に自動的に作られます スケジューリング システムには多数のスレッドが同時に存在します 多くの場合ではCPUの数 OSの役割は適切にCPUを割り当てることで,そのためにスケジューリングが必要になります スケジューリングの目標 公平性 独占的に割り当てないようにします 公平にCPUに割り当てます 効率性 実行可能なスレッドがある限り, CPUが休まないように動かせます スレッド切り替えのときのオーバーヘッドを少なくします 対話的プログラムの応答性 スケジューラの挙動を観察する いくつかのスレッドを立ち上げます スレッドがいつからいつまで実行中だったのかを調べます これには時刻を知るシステムコールが必要です 観察する点 どれくらいの頻度でスレッドが入れ替わっているのか その状態でのエディタやブラウザの応答性に影響はあるのか f(x) { t1 = currentTime(); while(1) { t2 = currentTime(); if (t2 - t1 > 1) { /* CPUをしばらく使われている */ } t1 = t2; } /* 記録を出力 */ } 時刻を知るシステムコール Unix: gettimeofday Win: QueryPerformanceCounter 特定のCPUへスレッドを固定 システムコール sched_setafinity(pid, size, mask) mask: size bitのbit列 maskで1となっているCPUでのみ,pidを実行 子プロセスへ自動的に継承 コマンド taskset -c command スレッドスケジューラの実現 クイズ 無限ループするプログラムを書いて実行してもマウスが動き,ブラウザが動き,(多くの場合)Ctrl-Cで消せるのはなぜ? 割り込み処理がされるから OSのないCPU クイズのような答えに成るのはCPU自体の動作は以下のようになっているからです CPUの状態はレジスタに入っています 汎用 プログラムカウンタの入っている命令を実行する CPUモードに応じて操作が制限されています CPUの動作は >> PC(プログラミングカウンタ)が指す場所の命令を取り出す命令を実行し状態を書き換える 割り込み処理 割り込みとはCPU外部からの信号のことです 割り込みの時にCPUが行うことは以下のようになっています if (割り込み許可中) { 割り込みを禁止する一部の状態(割り込み発生時PC(など))を特定のレジスタに保存割り込みベクタを参照し指定されている値をPCに設定(制御の移動) > } 割り込みベクタ 割り込みベクタとは割り込みの要因を示す番号のことです CPUに直接命令をします 割り込みベクタは通常OSの起動時に設定されます IRQ IRQとはキーバード･マウスなどの入出力デバイスがCPUを呼び出す時に生じる割り込み要求のことです IRQの通常の用途 入出力 キーボード･ネットワークコントローラ タイマ システムコール(トラップ命令) タイマ割り込み これはCPUの独占を防ぐための鍵です 通常定期的に発生させます(1-10ms程度に1回発生します) Linux(2.4) on x86, Windows on x86, BSDなどで10ms Linux on Alpha 1ms Linux 2.6.22以前は1ms, 4ms, または必要に応じて発生させます タイマ割り込み間隔･クロック間隔･クロックチックなどと呼び,必要な時にOSがCPUをアプリケーションから奪う機会を保証します 各スレッドの状態 中断状態 中断状態とは(CPUが空いていても)直ちに実行することができない状態のことです OSは中断状態のスレッドをCPU割当の対象から外します 中断と復帰 スレッドが中断する理由 入出力待ち(recv, read, etc) 自主的休眠(sleep) スレッド間の同期(pthread_join, wait, etc) ページフォルト 復帰の理由(中断の逆) 入出力完了 休眠時間経過 スレッド間の同期成立 ページフォルト処理完了 実行可能キュー 実行可能キューとは実行可能スレッドのリストのことです ｢スケジューリングキュー｣,｢ランキュー｣などという名前で呼ばれることがあります スレッドが実行可能キューにあるといことはそのキューが実行可能であるということです OSが機会あるごとに実行可能キューから最も適切なスレッドを選んで実行することをrescheduleと呼びます 中断 ネットワークからのデータ待ちによる中断を例とします recv() { ... if (読むべきdataがない) { 現スレッドを実行可能キューから外す reschedule() } ... } 中断からの復帰 ネットワークからのデータ到着による復帰を例とします /* 割り込み→OSのネットワークからの入力を処理 */ if (あるスレッドが今到着したデータ待ち) { そのスレッドを実行可能キューに入れる reschedule() } Rescheduleの機会 OSが制御を得るあらゆる時点が潜在的なrescheduleの機会です タイマ割り込み時 クロック間隔に一度 その他の割り込み(入力)からの復帰時 実行スレッドが中断した時 システムコードからの復帰時 etc rescheduleの機会をOSが得るたびに｢次に実行すべきスレッド｣を選択して実行します.つまりCPUの割当をしています 次に実行すべきスレッドの選択の仕方が重要です 公平性 効率性 対話的プログラムの応答 対話的なスレッドの応答性 エディタ,ブラウザ,メーラに｢キー入力｣をしたらすぐに反応してほしいです これに対応する対話的なスレッドは基本的には｢入力待ち｣です.そして以下の特徴が有ります I/Oによる中断が非常に頻繁 キーボード,マウス,ネットワークなどの入力待ち 中断が多いため,CPU利用量は少ないです 現在のLinuxスケジューラ カーネル2.6.23以降はCompletely Fair Scheduler(CFS)が使われています 特徴としては 各スレッドが使用したCPU時間を累積で管理しています vruntime(virtual runtime) reschedule時には毎回vruntimeが最小のスレッドを選びます つまり最もCPUを使っていないスレッドを選びます 公平性の保証をしています vruntime管理の実際 vruntimeは以下のように管理されています スレッドが生まれた時 子は親のvruntimeを引き継ぐ 子のvruntime = 0とはならない スレッドがAからBに切り替わる時 Aのvruntime += 今回消費した時間 Bのvruntime = そのまま しかしそれでは問題があります Bのvruntimeは最初0なとき,他のスレッドのvruntimeが100msだったときに100msも動かせます それではまずいのでBのvruntime (他のスレッドの最小vruntime) - 20msを保証します "},"os/os03.html":{"url":"os/os03.html","title":"03 C & Assembly","keywords":"","body":"03 C & Assembly 目標 Cプログラムの実行を機械語レベルで｢イメージ｣できるように成ることが目標です なぜやるのか 本来,OSやCPUは言語に関係ないです OSはプログラムがどの言語で書かれていようと関係のない言語で設計されています システムコール(API)の引数や返り値は整数やアドレスであり,Javaの配列やPerlの文字列ではないです CPUもあくまで機械語しかわかりません スレッドがメモリを共有をしているのはC言語の処理系がやっているのではなくCPUとOSが提供しています 機械語ですべてやるのが正しいですが,それでは不必要に複雑になるという理由でC言語で説明します.また, システムコールのAPI説明はCの関数で説明するのが慣習 スレッドがメモリを共有していることをC言語の変数や配列が共有されていることとして説明するのが慣習 C言語を理解するメリット 初級 ポインタを理解できます mallocなどアドレス関連の関数を理解できます Segmentation Faultを理解できます 中級 ポインタと配列の関係を理解できます 関数呼び出しの実装に関して理解できます 上級 C/C++言語ならではの理解不能バグを理解できます 中心的なテーマ Cプログラムによるメモリの使われ方について学びます 変数,配列がどこに格納され,どのようなCの式,文がどのようにメモリをアクセスされるのか 関数呼び出しの仕組み ローカル変数の格納場所(スタック) 呼び出しの入れ子の実現 制御構造,副プログラム,名前空間など CPU & メモリ コンピュータはレジスタと呼ばれるCPU内の少量の記憶領域とメインメモリと呼ばれるCPU外の記憶領域を用いて計算を行う機械です 主記憶にはプログラムが使うあらゆるデータが格納されています ブラウザで開いているタブ,Terminalなど C言語の変数,配列など 主記憶は単なるバイト列です 主記憶へはアドレスを指定して記憶場所を指定します 番地は単なる整数です 番地として許される値にはある一定の制限が有ります Cのプログラムもメモリをアクセスしているだけと言うことが可能です そのため,｢常に変数,配列,などなどあらゆる記憶域がメモリのどこかにプログラムが正しい限り重ならないように格納されている｣ということを考慮してください ポインタ ポインタとはつまりアドレスのことです char* p = ...; pに格納されているものはアドレス(整数) *pはpに格納されているアドレスをアクセスします p[5]はpに格納されているアドレス+5番地をアクセスしますす p[0]は*pの糖衣構文(syntax sugar)です 実践 int main() { char* p = 918; return *p; } このプログラムを実行するとエラー文が出てきます.それはOSをが間違った･悪意のあるプログラムからシステムを保護するために動いたからです(メモリ保護) 一般的な用語としてメモリ保護違反をすることにはSegmentation FaultというUnix用語を使います 変数のアドレスを見るには以下のようなプログラムを書きます &x; // xの値が格納されているアドレス #include int s; // 単純な変数 typedef struct point{ int x; int y; } point; point p; // 構造体の変数 int a[10]; // 配列 int main() { printf(\"%d %d %d %d %d %d\\n\", &s, &p.x, &p.y, &a[0], &a[5], &main); } 実はアドレスなもの int x &x; // 変数のアドレス int a[10]; a; // 配列 int* s = \"abc\"; s; // 文字列 int* q = malloc(100); q; // メモリ割り当て関数の返り値 int* p = 918; p; // 実はアドレスは整数値 C言語と他の言語の違い どんな言語も究極的には機械語(整数･浮動小数点しかない)です.つまり,メモリに色々なものをおき,そのアドレスで表している仕組み自体は同じです C言語の特徴はその｢仕組み｣をポインタという形で包み隠さず見せているところです.混乱の原因でもあり,自然かつ単純なところでもあります ポインタと配列の違い /** アドレス1個文の領域を確保 * そこに勝手なアドレスを格納できます * intを格納するための領域は確保されていません */ int* p; /** int 10個分の領域を確保します * aはその先頭のアドレスです */ int a[10]; ポインタに関する知識 int foo{ int a[100]; int* p; a[0] = 10; // OK p[0] = 10; // NG: どこにアクセスするか決まっていない } ポインタ変数へは初期化の必要があります int a[10]; int* p = a; // 以下は同じ a[0]; // p[0] a[3]; // p[3] (a+5); // (p+5) // 以下は異なります &a; // &p C言語で現れる3種類の記憶療育 大域(global)静的(static)変数･配列 関数外に書かれた変数･配列 関数中でstaticと書かれた変数･配列 局所(local)変数･配列 関数定義の中に書かれた変数定義 ヒープ malloc, new(C++)などで確保される 大域/静的 // 大域変数 int x; int a[10]; void foo() { // 静的変数 static int y; static int b[10]; } プログラム開始時に各変数,配列があるアドレスに割り当てられ,プログラム終了までその領域はその変数,配列のために確保され続けます つまり,そのアドレスは他の目的には使われません 局所 int fib(int n) { if (n fib(10)とfib(9)ではxは別の領域である必要が有ります 実際にどのように実装されているかと言うと,スタックというデータ構造を用いて,関数が呼び出された時にその呼び出しの実行のための領域を見つけて確保します スタックとは上から新しいものを入れて,上から取り除くようなデータ構造です 実際には以下のようなアルゴリズムでスタックに変数が格納されます 関数が実行を開始する時 その関数が使う局所変数の大きさに応じて空き領域からメモリを確保します 関数が終了する時 開始時に確保した分だけメモリを解放します 確保･解放とはスタックポインタをずらすことです 各スレッドが一つのスタックを持っています 局所変数･配列はそれを確保した関数呼び出しが終了すると解放されます スタックに関する知識 ほとんどのOS/CPUではスタックは大きい番地から小さい番地へ向かって伸びます 確保: SP -= size; 解放; SP += size; 小さい番地には命令列,大域変数,ヒープなどがありその方が都合が良かったのでは?と推測できます 現在ではスレッドが増えたので同じ手法ではできません また,スタックは無限に伸びないので使いすぎるとスタックオーバーフローを起こします.原因は以下の通りです 巨大な局所配列 深すぎる関数呼び出しの入れ子 スタックの大きさはスレッド生成時に指定できます 戻り番地 戻り番地とは関数終了後にジャンプする番地のことです スタックにはこの戻り番地が格納されています 戻り番地が破壊されることをバッファオーバランと言います デタラメな番地に行ってしまうので,セキュリティホールの原因となります しかし,デタラメな番地で済まずに攻撃する側が実行した命令が実行されるということも... ヒープ 任意の時点で確保,解放ができる領域をヒープと言います 確保 解放 大域変数･配列 静的変数･配列 プログラム開始時 されない(プログラム終了時) 局所変数･配列 関数開始時 関数終了時 ヒープ 任意(malloc, new) 任意(free, delete) Type* p = (Type*)malloc(size); // sizeバイト確保 ... free(p); // 解放 Type* p = new Type; ... delete p; malloc関連の間違い 1 freeをした後に確保した領域を使ってしまう p = malloc(...); ... free(p); // 早すぎるfree ... q = malloc(...); // その後,運悪く同じ番地が他のデータに割り当てられる ... *p = 10; // 意図しないデータを破壊 2 freeをし忘れてしまう char* p = (char*)malloc(...); ...*p = ...;...;...=*p; // ここまでで終了したつもり ...; char* q = malloc(...); ...; // しかし永遠に再利用されない 間違い探し 以降は二次元の点を表す構造体と,新しい点を作る関数の関数を例にしている Point* mk_point(int x, int y) { Point* p; // メモリの確保がされていないのでメンバ変数に代入できるかわからない p->x = x; p->y = y; return p; } Point* mk_point(int x, int y) { Point p[1]; // 局所変数なので関数実行後に解除される p->x = x; p->y = y; return p; // 関数が終わる時に解放される } Point* mk_point(int x, int y) { Point p; // これも上と同様に局所変数なので実行時に解放される p.x = x; p.y = y; return &p; } Point p; Point* mk_point(int x, int y) { // 大域変数に代入するだけなので何回実行しても同じ領域に代入しているだけ p.x = x; p.y = y; return &p; } 正解は次のように書きます Point* mk_point(int x, int y) { Point* p = (Point*)malloc(sizeof(Point)); p->x = x; p->y = y; return p; } 間違いの類型化 割り当てられた領域と関係のない場所をアクセスする char* p = 918; // 918番地を確保してている ... char* p; // 初期化し忘れ ... 割り当てられた領域をはみ出してアクセスする char a[100]; a[100] = ...; // a[0]~a[99]までしかアクセスできない char* a = (char*)malloc(10); a[10] = ...; // 上と同じ 寿命後のアクセス 2回freeをする freeをした後に領域にアクセスしてしまう これらの間違いを犯さないためには もう使われない領域を再利用しない 解放が遅かったり,しないことが起きます が重要です.プログラムの終了までは大した量のメモリが使われなければあまり問題ではありません 長時間実行すると,実行が急に遅くなるなど突如問題に成ります 安全な言語 安全というのはメモリを正しく使い,エラーを起こさないか検出するようなものです これをメモリ安全とかポインタ安全と言います 大まかな定義としては 配列･構造体･文字列などとして割り当てられた場所以外を読み書きできない 配列･構造体･文字列などとして割り当てた場所を読み書きするにはその配列･構造体･文字列への参照を得なければならない を満たす言語です "},"os/os04.html":{"url":"os/os04.html","title":"04 Parallelism & Synchronization","keywords":"","body":"04 Parallelism & Synchronization スレッドとプロセス プログラムはCPUの数だけ同時に実行され,CPUを超えるスレッド･プロセスはOSによって交互に実行されます 性能の上げ方･目的 並列処理 マルチコア化 I/O遅延隠蔽 I/Oによりブロックするスレッドを複数実行してCPU利用率を向上 記述の簡潔化 論理的にほぼ独立なタスクを簡潔に記述化 協調 問題データや計算結果の受け渡しのためにスレッド･プロセス間では通信が行われています 通信方法としては プロセス間: ファイル,パイプ,ソケット... スレッド間: 上記 + 共有メモリ 共有メモリ 1つのプロセスの中の複数のスレッド間でメモリが共有されます これはCPUが提供している機能で,あるアドレスに書き込んだ結果はどのCPUからでも読み出せます(OSや言語処理系は関係ない) 競合状態 並行して実行されているスレッド･プロセス間の危険な相互作用として 共有メモリの更新と読み出し 共有ファイルの更新と読み書き があります.共有メモリ(スレッド)を用いてスレッド(プロセス)の間で正しく通信することは自明でありません 以降では共有メモリを介して通信するスレッド間の協調に話を絞ります 注意点 競合状態にならないように気を付ける点は以下の通りです 順序(依存関係)は確保されているか? Aの後にBを実行する場合に,その順番は守られているか? 不可分性,原子性(atomicity)は確保されているか? AとBが連続して実行される場合に,AとBは途切れずに実行されているのか? 以下では用語の説明をします 競合状態(race condition): スレッド間の危ない相互作用が起きる状態です 同じ領域を複数スレッドが並行にアクセスし,少なくともどちらかは書き込みをしている クリティカルセクション(critical section): 強豪が発生している(時間的)区間 同期: 競合状態を解消するためのタイミングの制御 勝手なスケジューリングを許さないための制約です Mutex(排他制御) Mutexとはlockで他のスレッドやプロセスが実行できなくなり,unlockでその状態が解除されるような同期プリミティブです 多くの場合は他のスレッドと共有しているデータの読み出しから更新までを不可分に行いたいときに使われます "},"other/editor.html":{"url":"other/editor.html","title":"Editor","keywords":"","body":"Editor "},"other/latex.html":{"url":"other/latex.html","title":"Latex","keywords":"","body":"Latex "},"other/git.html":{"url":"other/git.html","title":"Git","keywords":"","body":"Git "}}