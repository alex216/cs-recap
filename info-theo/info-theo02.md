# 02 Information Resource

## 情報源のモデル

今まで確率的に生起する事象を扱ってきましたが,実際の事象は一回で終わらず事象が系列となり,次々と起こります

しかも,これから出てくる事象は今までに発生した事象に依存して確率的に定まることが多いです

このようにこれから出てくる情報が過去の情報に依存して確率的に定まるものを**情報源**と言います

情報源が時刻1ごとに1つの事象を発生するとします

情報源は$k$個の事象$A_1, A_2,\ldots,A_k$を手持ちとして,このうちの一つを出すと考えます

天気の場合,事象それぞれが天候の名称です.$A_1=\text{晴れ}$, $A_2=\text{曇り}$とすると事象$A_1$が連続して何回も起こるというのは確率的に難しいでしょう(もちろんここでは単純化して話しています)

### 英語の例(情報源のモデル)

英語の場合,情報源の手持ちの事象にはアルファベット26文字と句読点があり,これらが確率的に出ていると考えれば良いです

### 一般化

以降では$k$個の事象\ldotsと言う代わりに現れる文字の全体$\{A_1,A_2,\ldots,A_k\}$のことをアルファベットと呼びます

天気の場合は｢晴れ｣や｢曇り｣など天候の名称がアルファベットです

時刻$t$($t=1,2,3,\ldots$)に発生する文字を$x_t$で表すとします.これが$A_i$ならば,

$$
x_t=A_i
$$

です

それ以前に出てきた$s$個($s=1,2,3,\ldots$)の文字の系列が

$$
x_{t-s}\ldots x_{t-2}x_{t-1}
$$

であるとき,時刻$t$に文字$x_t$の出る確率は

$$
p(x_t \mid x_{t-s}\ldots x_{t-1})
$$

と書くことにします

また,時刻$t-s\sim t$に文字系列

$$
x_{t-s}\ldots x_{t-1}x_t
$$

が出る確率は

$$
p(x_{t-r}\ldots x_t)
$$

と書くことにします

このとき

$$
p(x_{t-s}\ldots x{t-1}x{t}=p(x_t \mid x_{t-s}\ldots x_{t-1})p(x_{t-s}\ldots x_{t-1})
$$

です

これで文字系列を発生し続ける情報源の確率的な性質がわかります

### 定常的な情報源

ここでは情報源の性質が変わらないことを仮定しています

例えば,情報源が英語から日本語に変わってしまったらこれまでの文字系列が持つ意味はほとんど意味がなくなってしまいます

このような情報源の性質が時間が経っても変わらないことは,条件付き確率$p(x_t \mid x_{t-s}x_{t-1})$は$t$がなんであっても同じです

つまり$r=1,2,\ldots$に対して

- $p(x_{t+r})=p(x_t)$
- $p(x_{t+r} \mid x_{t+r-s}\ldots x_{t+r+1}) = p(x_t \mid x_{t-s}\ldots x_{t-1})$

を意味します

このような情報源のことを**定常的な情報源**と言います

## 情報源のエントロピー

時刻$t$に文字$x_t$が出る確率は$p(x_t)$だから,このエントロピーは

$$
H(X_t)= -\sum p(x_t)\log p(x_t)
$$

で与えられます

大文字$X_t$は時刻$t$の$x_t$を表す確率変数です

情報源は定常的だからこれは

$$
H(X) = - \sum_x p(x)\log p(x)
$$

と書けます.また

$$
H(X) = \sum_i p(x=A_i)\log p(x=A_i)
$$

と書けます

これは情報源1文字あたりのエントロピーではありません.なぜなら情報源から出てくる文字の系列を考える場合,$x_t$は$x_{t-1}, x_{t-2},\ldots$の文字と相互に影響していてその影響を無視できないからです

そこで,情報源から出る相続いた2文字$x_1 x_2$を見ます

2文字のエントロピーは

$$
H(X_1 X_2)= - \sum_{x_1 x_2}p(x_1 x_2)\log p(x_1 x_2)
$$

と書けます

2文字の間に相関がある場合,このエントロピーは文字を1つずつ独立に見たときのエントロピー$2H(X)$よりは小さいです.つまり

$$
H(X_{1}X_{2}) \leq 2H(X)
$$

同様に,$n$個の相続いた文字の系列を見たときのエントロピーは

$$
H(X^n) = - \sum_{x^n}p(x^n)\log p(x^n)
$$

です.ただし,ここでは

$$
\begin{aligned}
X^n &= X_1 X_2\ldots X_n\\
x^n &= x_1 x_2\ldots x_n
\end{aligned}
$$

です

ここでは$n$個で$H(X^n)$のエントロピーを1文字ずつに割り振ったとき

$$
H_{n}=\frac{H(x^{n})}{n}
$$

が1つ当たりのエントロピーとなります

$n$個の文字$x^n=x_1 x_2\ldots x_n$の間に相関がなければ,これは$H(X)$に等しいが,相関があれば$H_n$は$n$と共に減少していきます

これは｢日本大学経済学｣の次の文字が｢部｣だと予想がつくことからもわかるでしょう

ここで情報源の1文字当たりのエントロピー$H$を

$$
H=\lim_{n \to \infty} H_n
$$

と定義します.これは情報源から出る文字系列のずっと長い間見続けたいときの1文字当たりのエントロピーです

次のようにも考えられます.今までに出た$n$個の文字を覚えているとして,次の文字がなんであるかと考えるときのエントロピー,条件付きエントロピーは

$$
H(X \mid X^n) = - \sum p(x^n) p(x \mid x^n) \log p(x \mid x^n)
$$

となります.このとき情報源のエントロピーを

$$
\lim_{n \to \infty}H(X \mid X^n) = H(X \mid X^{\infty})
$$

で定義します.これも$n$と共に単調減少します

これは先に定義した$H$に収束します

この証明は

$$
H(X^n)=H(X)+H(X \mid X)+H(X \mid X^2)+\ldots+H(X \mid X^{n-1})
$$

からすることが可能です

### 英語の例(情報源のエントロピー)

英語のアルファベットの26個が全て等確率で独立に発生すると仮定すると,このエントロピーは

$$
H_0 = \log_2 26 = 4.7\text{ビット}
$$

です

実際の出現頻度を調べた結果を元にすると

$$
H_1 = 4.15\text{ビット}
$$

となります.しかしこれも前後の文字の影響が入っていないのでそれも考慮すると,英語の2文字の組み合わせ$x^2=x_1 x_2$の全ての頻度を調べ,$H_2$を求めると

$$
H_2 = 3.57\text{ビット}
$$

となります.同様に8文字の文字系列$x_8 = x_1 x_2 \ldots x_8$の全ての出現頻度を調べると

$$
H_8 = \frac{1}{8} H(X^8) = - \frac{1}{8}\sum p(x^8)\log p(x^8)
$$

と計算できるようです.これによると

$$
H_8 = 2.35 \text{ビット}
$$

となるようです

しかし,現実では文章の始めの方では話の予想はつかないが,最後の方では予想が付くことが多いです(推理小説など)

そのため,大方の予想では

$$
H=1.3\text{ビット}
$$

程度ということになっているそうです
