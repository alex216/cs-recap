# 01 Entropy

## 情報量

情報を知識を｢不確実｣なものから｢確実｣なものへとするものと定義します

その際

$$
\text{情報量} = \text{不確実さ}
$$

と式で表せます

事象が$n$個あり,その中から$1 \leq i \leq n$として$A_i$が起きるのを知らせる情報量を求める関数を$f(n)$とします

$n$個が大きいほど,どの事象が起こったのか見当がつきにくいので$f(n)$は大きくなります

また,｢$n$個の事象を$k$個ずつ$m$組に分けて,その$m$組の中から1つを選ぶ,その組の$k$個の中から1つを選ぶ｣場合と｢直接$n(=m+k)$個の事象から1つを選ぶ｣場合で得られる情報量は同じです.

つまり

$$
f(mk) = f(m) + f(k)
$$

が成り立ちます

これを**情報の加法性**と言います

### ビット

ここで$f(xy)=f(x)+f(y)$とすると

$$
f(x + \varepsilon x) = f\{(1 + \varepsilon)x\}\\
= f(1 + \varepsilon) + f(x)
$$

よって

$$
\frac{f(x+\varepsilon x) - f(x)}{\varepsilon x} = \frac{f(1+\varepsilon)}{\varepsilon x} \tag{1}
$$

$\varepsilon \to 0$とすると左辺は明らかに$f(x)$の導関数$f'(x)$であり,

$$
\lim_{\varepsilon \to 0} \frac{f(1 + \varepsilon)}{\varepsilon} = c
$$

とするとこれを変形して

$$
\lim_{\varepsilon \to 0} \frac{f(1 + \varepsilon)}{\varepsilon x} = \frac{c}{x}
$$

であり$(1)$から左辺は$f'(x)$だから

$$
f'(x) = \frac{c}{x}
$$

という微分方程式を得る.両辺を$x$で積分して

$$
f(x) = c\log(x) + d \;\;\;(\log = \log_e)
$$

となる

$n=1$のとき$f(1)=0$であるから$d=0$

YES or Noで決まるような二者択一の情報量の単位を**1ビット**(bit, binary digit)とすると

$$
f(2) = 1\text{bit}
$$

よって$f(x)=c\log(x)$に代入して

$$
f(2) = c\log_e 2 = 1
$$

つまり

$$
c \cdot \frac{\log_2 2}{\log_2 e} = 1
$$

よって

$$
c = \log_2 e
$$

以上より$f(x) =\log_2 x \text{ bit}$

また以降対数の底は$2$とする

### 一般化

上の話の一般化をする

ある事象$A$が起きる確率を$p$とします.このとき$p$を有理数として$p=\frac{k}{n}$とする

$A$は$n$個の等確率で起きる事象のうち$k$個をひとまとめにしたものとする.このとき,$A$が起こるのは$k$個のうちどれか1個が起こることだとすると確かに$p$は上のような確率となる

$n$個のうちどれかが起こったことを知るための情報量は$\log n$である

$A$が起こったことを教える情報量を$I$とすると,$I$だけでは$k$のうちどれが起こったかはわからないから,更に$\log k$の情報量が必要となる

つまり情報量$I$は

\begin{eqnarray}
\log n &=& I + \log k\\
I &=& \log n - \log k\\
&=& -(\log k - \log n)\\
&=& - \log \frac{k}{n}
\end{eqnarray}

となります

また,$A_1,A_2,..,A_n$は全て確率$\frac{1}{n}$で起きるので,このうちどれかが起こったのか知らせる情報量は

$$
-\log \frac{1}{n} = \log n
$$

以上より

**定義**: 確率$p$の事象が実際に起こったことを知らせる情報に含まれる情報量は

$$
-\log_2 p \text{ ビット}
$$

これは$p$が小さいほど大きくなります.起きにくいことが起きると知った時の方が確かにおどろくという実態とマッチしているように感じます

## エントロピー

情報量は**起こったことを知らせる値**ですが,エントロピーは**不確定な状況を確定するのに必要な情報量の値**です

具体的には$A_1,A_2,..., A_n$の$n$個の事象の確率が$p_1,p_2,..,p_n$だとして$\sum_{i=1}^{n}p_i=1$であり,得られる情報量の期待値$I$は以下の通りです

$$
I = - \sum_{i=1}^{n}p_i \log p_i
$$

この値は**エントロピー**です

**定義**: $n$個の事象の確率がそれぞれ$p_1,p_2,...,p_n$で発生する時どれが発生したかの不確定度(エントロピー)は

$$
H(p_1,p_2,...,p_n)=-\sum_{i=1}^{n}p_i \log p_i
$$

エントロピーは以下の性質を持ちます

* エントロピーは非負
* $n$個の事象を表すエントロピーの最大値$H(n)$は$H(n)=\log n$で,全ての事象が等しい確率で起こるときの不確定度
* 情報を得て情報エントロピーが$H$から$H'$に変わったときにこの情報量を$I=H-H'$とする

### 複合事象のエントロピー

**複合事象**とは複数の事象が組み合わさったものです

例えば事象$A_i$と$B_j$の複合事象の不確定度を示すエントロピー$H(A,B)$は

$$
H(A, B) = - \sum_{i,j}p(A_i, B_j)\log p(A_i, B_j)
$$

### 条件付きエントロピー

$$
\begin{cases}
\text{同時確率分布: }p(A_i, B_j)\\
\text{周辺確率分布: }p(A_i), p(B_j)\\
\text{条件付き確率分布: }p_{A_i}(B_j), p_{B_j}(A_i)
\end{cases}
$$

確率分布は上のように分類できます

条件付き確率分布は例えば$p_{A_i}(B_j)$は$A=A_i$のときの$B= B_j$となる確率のことです

これらの確率分布は以下のような特徴を持ちます

1. 
\begin{eqnarray}
\sum_{i,j}p(A_i,B_j)&=&1\\
\sum_{i}p(A_i)&=&\sum_{j}p(B_j)=1\\
\sum_{j}p_{A_i}(B_j)&=&\sum_{i}p_{B_j}(A_i)=1
\end{eqnarray}

   すべての確率を足したら$1$になるということです

2. 
$$
p(A_i)=\sum_{j}p(A_i,B_j)\\
p(B_j)=\sum_{i}p(A_i,B_j)\\
$$

   同時確率分布で片方を固定した場合の和は固定された方の周辺確率分布になるということです

3.
$$
p(A_i,B_j)=p(A_i)p_{A_i}(B_j)=p(B_j)p_{B_j}(A_i)
$$

   $A_i, B_j$の同時確率分布は$A_i$の確率と$A_i$が起きたときの$B_j$の確率を書けたものです

4. 
$$
p_{A_i}(B_j)=\frac{p(B_j)p_{B_j}(A_i)}{p(A_i)}=\frac{p(A_i,B_j)}{p(A_i)}\\
p_{B_j}(A_i)=\frac{p(A_i)p_{A_i}(B_j)}{p(B_j)}=\frac{p(A_i,B_j)}{p(B_j)}
$$

   これは条件付き確率の定義です.3から容易に求めることができます 
   
   Bayesの公式とも言われています

   3, 4は$A_i,B_j$が独立の時に成り立ちます

$A$が何であるかを知ったときの$B$が何であるかについての不確定度を求める

$A$が$A_i$であるとわかったときの不確定度を表すエントロピーは

$$
H_{A_i}(B)=-\sum_{j=1}^m p_{A}(B_j)\log p_{A}(B_j)
$$

求めるのは$A$が何であるかは不確定であり,求めるエントロピーは全ての$A_i$について平均したものであるから

\begin{eqnarray}
H_A(B)&=&\sum_{i=1}^{n}p(A_i)H_{A_i}(B)\\
&=&-\sum_{i,j}p(A_i)p_{A_i}(B_j)\log p_{A_i}(B_j)
\end{eqnarray}

このエントロピーの平均値$H_A(B)$を**条件付きエントロピー**と言う

条件付きエントロピーもいくつかの性質を持つ

1. 
\begin{eqnarray}
H(A,B)&=&H(A)+H_A(B)\\
&=&H(B)+H_B(A)\\
H_A(B)&=&H(A,B)-H(A)
\end{eqnarray}

   最終的に同じことが分かっていれば同じ情報量になるということです

2. 
$$
H_A(B) \leq 0
$$

   エントロピーは非負であるので,その平均値も非負です  
   等号が成立するのは$H_{A_i}(B)$が全て$0$になるときです

3. 
$$
H(A)+H(B) \leq H(A,B)
$$

   $A$と$B$が独立であるときに統合は成立します

4. 
$$
H(A) \geq H_B(A)\\
H(B) \geq H_A(B)
$$

   これは情報の不確かさが何か情報が与えられて増える

5. 
$$
H(A,B)=H(A)+H_A(B)\\
=H(B)+H_B(A)\\
H_A(B)=H(A,B)-H(A)
$$

   $A$と$B$についての情報を知っている方が片方だけを知っているよりも大きいということです

### 相互情報量

$A$と$B$に関係がある時,$A$についての情報を得れば$B$についていくらか知ることができます

この情報量を**相互情報量**$I(A,B)$とします

$$
I(A,B)=H(B)-H_A(B)
$$

相互情報量は次のような性質を持つ

1. 
$$
I(A,B)=I(B,A)
$$

$I(A,B)=H(B)-H_A(B)=H(B)+H(A)-H(A,B)$であり,$H(B)-H(A,B)=H_B(A)$であるということからわかります

$A$と$B$は同じように関係しているので片方を知ることで得られるもう片方への情報量は同じだということがわかる

2. 
$$
I(A,B)\leq H(A),H(B)
$$

   定義より明らかです

3. 
$$
I(A,B)\leq 0
$$

   $A$を知り$B$についての情報が増えることはあっても減ることはないので理解できると思います

4. 
$$
I(A,B|C) \geq 0
$$

   ここでは3つの事象系に関する情報量を扱っています
   
   $I(A,B|C)=\sum_{i,j,k}p(C_k)p_{C_k}(A_i,B_j)\log\frac{p_{C_k}(A_i,B_j)}{p_{C_k}(A_i)p_{C_k}(B_j)}$と定義します
   
   これは$C=C_k$だとわかったときの$A$と$B$との相互情報量の平均値で,$C$がなんであるかわかったときに$A$を知って得られる$B$に関する情報量です.これが$0$より大きくなるのは上と同じ理由からです

5. 
$$
I(AB,C)=I(A,C)+I(B,C|A)
$$

   $AB$を知ってから$C$について得る情報量は,$A$を知ってから$C$について得る情報量と,$A$を知った後で$B$を知ってから$C$について得る情報量を足した情報量と等しいということです

    
6. 
$$
I(AB,C) \geq I(A,C)
$$

   これは$A$だけでなく$B$についても知って得る情報量が減るということはないです

7. 
$$
I(A,B,C)=I(B,C,A)=...
$$

   相互情報量は関係のある事象系の数が増えても対称性が成り立つということです
   
   これは相互情報量の定義から当然なはずです